Date,Post score,Post title,Body of the post,Top comment 1,Top comment 2,Top comment 3
1706704645.0,1707,Friendly reminder not to work too hard. You'll just get fired,"The year just started and there are already over 50K layoffs. The latest one is UPS, including some data professionals at corporate. These are people who worked hard, built a career with the company over extremely long period of time, stayed loyal, 3% merit increases, worked extra hours because they believed that they were contributing to a better future for the company and themselves.... And they were laid off without a second thought for cost saving. Yeah, Because that makes so much sense, right? Record-breaking profits every year is an unattainable goal, and it's stupid that here in the USA, we are one of the only countries that keeps pushing for this while other countries are leaving us in the dust with their quality of life....


So just remember. If you're thinking about doing some overtime for free, or going above and beyond just for a pat on the back, don't do it. You only have so many years on Earth. Focus on your own life and prioritize yourself, always","I'm in HR Workforce management,

Let me tell you, C-Suite usually only ask for Salaries by job profiles. That's it. No engagement scores, performance metrics, Management reviews. Nada. They rarely dig into the specifics of who you are as a person, rather the role you play in the company and what roles are ""expendable"".

It blows my mind how unscientific and how broad the brushstrokes are when they layoff people. It's literally the most awkward and clunky way to save money. It's as if Finance went through a car and started tearing up things they don't need with 0 knowledge of how a car actually works. For the most part they will 100% save money, but in the long term, that car will fall apart and the remaining pieces are overworked and fail sooner. Taking off hubcaps, spare tires, air caps....","I agree. I'm going through something similar as well. Worked my ass off to help build a company since 2019, and then throughout the pandemic, for that company to just say that my performance is poor last year and they are unable to give me any promotion or increment.

Then, they switched my job scope to something outside of my role (I know, this is a way for them to ""fire"" me without firing me). And I couldn't take it, so I decided to quit, and the company was happy to sign me off.

5 years of hard work, day and night, and on weekends too (especially during the pandemic when everyone was stuck inside) and it all summed up to them quietly kicking me out.","I lost my job three times in a row, so I think loyalty is overrated."
1709037427.0,1387,Data scientist quits her job at Spotify,In summary and basically talks about how she was managing a high priority product at Spotify after 3 years at Spotify. She was the ONLY DATA SCIENTIST working on this project and with pushy stakeholders she was working 14-15 hour days. Frankly this would piss me the fuck off. How the hell does some shit like this even happen? How common is this? For a place like Spotify it sounds quite shocking. How do you manage a ‚Äúpushy‚Äù stakeholder?,You can‚Äôt manage a pushy stakeholder unless your boss supports you.,"> How do you manage a ‚Äúpushy‚Äù stakeholder?

I'm a SWE, not a data scientist, but my answer for a case like this is quite simple.

Me: ""I'm struggling to meet the demands here. My understanding is that this is a very important and high priority initiative, is that right?""

Stakeholder: ""Yes, it's very high priority and urgent.""

Me: ""So why am I the only person who's working on it? Can we get more resources?""

Then the stakeholder is forced to either backtrack and admit that the urgency and priority isn't as high as they're making it out to be, or else is forced up the chain of command to provide additional resources.

If this seems confrontational, _it is_. If you work at a place where you feel afraid to have a direct conversation like this, you need to get out regardless.","This is not uncommon, especially for data scientists embedded into non-data teams as she was. Ultimately, stakeholders dont understand the time silent work takes (interpreting requirements, sourcing or even extracting data, due dilligence in data interpretation, data processing, storytelling, generating visualizations) as its outside their domain and have their own prios/deadlines to worry about. Tbf, I‚Äôm a DS and cant even properly estimate how long a request will take unless using data I produced or am super familiar with.¬† ¬†¬† ¬† ¬†¬†

Add on top, managing changing requirements and the enormous level of context switching between trying to do deep work and sporading meetings in all levels of the org (your immediate stakeholders, your team, company-wide, etc) and Slack messages.¬†So, data scientists are asked for work beyond the time or mental bandwidth they have.¬†¬† ¬† ¬† ¬†¬†

The DS role can be VERY ambiguous and unstructured (even in good companies like Spotify), stories like this are the unfortunate consequence. I‚Äôm personally preparing myself to transition to slightly more structured job types within tech because each of my DS roles is completely different from the next, including in the tasks I do or the knowledge I‚Äôm expected to have."
1708323275.0,1056,The BS they tell about Data Science‚Ä¶,"1. In what world does a Director of DS only make $200k, and the VP of Anything only make $210k???

2. In what world does the compensation increase become smaller, the higher the promotion? 

3. They present it as if this is completely achievable just by ‚Äúfollowing the path‚Äù, while in reality it takes a lot of luck and politics to become anything higher than a DS manager, and it happens very rarely. ",Wait... who's paying Analysts 90küßê,The real bs is we can start right here lol,Meanwhile to anyone outside of the US these salaries are absolutely insane. Glassdoor says the average DS in the UK makes ¬£52k which is around $65k.
1709658865.0,888,Everything I've been doing is suddenly considered AI now,"Anyone else experience this where your company, PR, website, marketing, now says their analytics and DS offerings are all AI or AI driven now?

All of a sudden, all these Machine Learning methods such as OLS regression (or associated regression techniques), Logistic Regression, Neural Nets, Decision Trees, etc...All the stuff that's been around for decades underpinning these projects and/or front end solutions are now considered AI by senior management and the people who sell/buy them. I realize it's on larger datasets, more data, more server power etc, now, but still.

Personally I don't care whether it's called AI one way or another, and to me it's all technically intelligence which is artificial (so is a basic calculator in my view); I just find it funny that everything is AI now.",Ride the wave üåä just make sure to have savings when it crashes,"IMO, the greatest achievement of AI is companies convincing others that everything is the product of AI.","10+ years ago it was all ""Big Data"".

Most of it still is.

And much it was also AI at the time.  The technical definition of AI is pretty broad."
1710119472.0,798,Turns out my best data science work is helping Redditors get jobs...,,I‚Äôve mentored a few friends into getting better jobs and careers than myself. They were starting at nothing. Somehow I can‚Äôt figure out how to do this for myself.,Are you looking to help more people lol?,What would be your advice for a new grad seeking roles in the market?
1710926800.0,782,A data scientist got caught lying about their project work and past experience during interview today,"I was part of an interview panel for a staff data science role. The candidate had written a really impressive resume with lots of domain specific project work experience about creating and deploying cutting-edge ML products. They had even mentioned the ROI in millions of dollars. The candidate started talking endlessly about the ML models they had built, the cloud platforms they'd used to deploy, etc. But then, when other panelists dug in, the candidate could not answer some domain specific questions they had claimed extensive experience for. So it was just like any other interview.

One panelist wasn't convinced by the resume though. Turns out this panelist had been a consultant at the company where the candidate had worked previously, and had many acquaintances from there on LinkedIn as well. She texted one of them asking if the claims the candidate was making were true. According to this acquaintance, the candidate was not even part of the projects they'd mentioned on the resume, and the ROI numbers were all made up. Turns out the project team had once given a demo to the candidate's team on how to use their ML product.

When the panelist shared this information with others on the panel, the candidate was rejected and a feedback was sent to the HR saying the candidate had faked their work experience.

This isn't the first time I've come across people ""plagiarizing"" (for the lack of a better word) others' project works as their's during interview and in resumes. But this incident was wild. But do you think a deserving and more eligible candidate misses an opportunity everytime a fake resume lands at your desk? Should HR do a better job filtering resumes?

Edit 1: Some have asked if she knew the whole company. Obviously not, even though its not a big company. But the person she connected with knew about the project the candidate had mentioned in the resume. All she asked was whether the candidate was related to the project or not. Also, the candidate had already resigned from the company, signed NOC for background checks, and was a immediate joiner, which is one of the reasons why they were shortlisted by the HR. 

Edit 2: My field of work requires good amount of domain knowledge, at least at the Staff/Senior role, who're supposed to lead a team. It's still a gamble nevertheless, irrespective of who is hired, and most hiring managers know it pretty well. They just like to derisk as much as they can so that the team does not suffer. As I said the candidate's interview was just like any other interview except for the fact that they got caught. Had they not gone overboard with exxagerating their  experience, the situation would be much different.","We hired a junior Analyst who leaned into data science at a Fortune 100 company. Incredibly bright but couldn't stick with an idea long enough to produce work product so we were starting a pip when they quit. 

Three years later, they are a VP at a Fortune 10. I've been sent their resume by a few colleagues who had him in their hiring pools for feedback. There isn't an ounce of truth from our time together. It lists management of teams that didn't exist, projects the individual never worked on, experience the person never demonstrated etc.

Yet it worked. This person had three jobs before getting the VP role but seems to be doing well for themselves.

It's why both technical and behavioral interviews are important.","I‚Äôve been in IT/data for 25+ years. From my anecdotal experience, your case is the norm and not an outlier. It‚Äôs human nature. 

It‚Äôs why we have phone interviews and in-person interviews. 

p.s. ROI numbers on resumes are the stupidest thing to happen in the last ten years.","I mean to be fair, many companies are also blatantly lying about their work culture, workload and other important facts. I guess it's all about navigating through the bullshit and lies in a interview process, which goes both ways. I always try to stay suspicious as a interviewer aswell as an interviewee."
1706085085.0,681,"Is it just me, or is matplotlib just a garbage fucking library?","With how amazing the python ecosystem is and how deeply integrated libraries are to everyday tasks, it always surprises me that the ‚Äúmain‚Äù plotting library in python is just so so bad.

A lot of it is just confusing and doesn‚Äôt make sense, if you want to have anything other than the most basic chart.

Not only that, the documentation is atrocious too. There are large learning curve for the library and an equally large learning curve for the documentation itself

I would‚Äôve hoped that someone can come up with something better (seaborn is only marginally better imo), but I guess this is what we‚Äôre stuck with","We rarely use matplotlib directly. If you use a library which uses matplotlib as a backend, then of course you have to deal with it a bit; but otherwise I strongly suggest to use plotly instead for your own visualizations.","If memory serves, matplotlib was essentially copied from the equivalent Matlab library because that was the most popular option at the time. They implemented a lot of things in the ""Matlab way"" to minimized transition costs which makes the interface awkward and non-Pythonic.

It's an old library that has a lot of momentum and a lot of legacy users. But you don't have to use it. Most people I know have long switched to other options. It's slowly fading away as the more modern libraries take over.","Yup, I agree. You can create amazing plots with matplotlib, but it's not going to be a fun experience. Plotly is better, seaborn also helps but is still dependent on matplotlib for many things.

I'm a Python guy, but ggplot2 is absolutely so much better to work with than any python plotting library."
1711097326.0,676,"DS Salary is mainly determined by geography, not your skill level","I have built a model that predicts the salary of Data Scientists / ML Engineers based on 23,997 responses and 294 questions from a 2022 Kaggle Machine Learning & Data Science Survey.

Below are the feature importances from LGBM.

TL;DR: Country of residence is **an order of magnitude** more important than anything else (including your experience, job title or the industry you work in).

Source: [https://jobs-in-data.com/salary/data-scientist-salary](https://jobs-in-data.com/salary/data-scientist-salary)



https://preview.redd.it/b89q4likmupc1.png?width=1200&format=png&auto=webp&s=3e989d7cb71601e45cd3bea86802c0a8294e9e9d","Well, yeah.

If a job exists in a country it tends to pay relative to the cost of living in that country, or at least relative to how much other jobs in that country pay.

Also, no. You generally can't just work remotely for a US company and get the geographic pay difference. They'll want to pay you based on where they owe taxes on your income.","This applies to every job so it isn't a surprise. An accountant in the US will usually earn more than the same profession in Nigeria (to pick a random place). Also most people can't easily move countries. You might as well remove geography from the data, and maybe we get something more insightful out of this.","That's true for every job because cost of living and economic strength of a country determine average salary level.

So what's the feature importance if we account for that and which countries do pay less/more for DS relative to their median salary?"
1708005545.0,642,A harsh truth about data science....,"Broadly speaking, the job of a data scientist is to use data to understand things, create value, and inform business decisions. It it *not necessarily* to implement and utilize advanced Machine Learning and Artificial Intelligence techniques. That's not to say that you can't or won't use ML/AI to inform business decisions, what I'm saying is that it's not always required to. Obviously this is going to depend on your company, their products, and role, but let's talk about a quintessential DS position at a quintessential company.

I think the problem a lot of newer or prospective Data Scientists run into is that they learn all these advanced techniques and want to start using them right away. They apply them anywhere they can, kind of shoehorning them in and not having a clear idea of what it is they are even trying to accomplish in the first place. In other words, the tools lead the problem. Of course, the way it should be is that the problem leads the tools. I'm coming to find for like 50+% of the things I'm asked to do, a time series visualization, contingency tables, and histograms are sufficient to answer the question to the satisfaction of the business leaders. That's it. We're done, on to the next one. Start simple, if the simple techniques don't answer the question, then move on to the more advanced stuff. I speak from experience, of course.

In my opinion, understanding when to use simple tools vs when to break out the big guns is way harder then figuring out how to use the big guns. Even harder still is taking your findings and translating them into actual, actionable insights that a business can use. Okay, so you built a multi-layer CNN that models customer behavior? That's great, but what does the business do with it? For example, can you use it to identify customers who might buy more product with more advertising? Can you put a list of those customers on the CEO's desk? Could a simple regression model have done the same in 1/4 of the time? These are skills that take years to learn and so it's totally understandable for newer or prospective DSs to not have them. But they do not seem to be emphasized in a lot of degree programs or MOOCs. It seems to me like they just hand you a dataset and tell you what to do with it. It's great that you can use the tools they tell you to on it, but you're missing out on the identifying which tools to even use part in the first place.

Just my 2c. 

&#x200B;",I also think a huge factor is that companies ask for AI and ML solutions because it's what they hear about and what they can brag about. That then pushes DS to use tools they don't need to.¬†,"Can't we just acknowledge that knowing to write code, understand statistics, and also domain expertise, are difficult skills to master, even if you don't need to invent novel algorithms?
The fact that the job is not ultra complicated is making PhDs who are used to get paid pennies for ultra difficult tasks perplexed.

Anyway, I am asked to develop novel models and honestly sick of the pressure and uncertainty a little.","The harsh truth is that it wasn‚Äôt always like that‚Ä¶ Here is a brief history of the data science title and how it shifted. TLDR at the bottom. 

What is the difference between a data scientist today and a statistical analyst of yesterday? Not much, maybe data scientists use python now and Jupyter notebooks instead of R and markdown files. But the original vision for the data scientist as described by Harvard‚Äôs infamous article describing data science as the sexiest job was not just a rebranding of a statistical analyst. 

Data science made its debut in product. The product was always the algorithm that helped automate decision making. Recommender systems, translation, most recently chat bots. These are the things that originally got us excited about data science when we were laymen. Even in the infamous article ‚ÄúData Scientist: the Sexiest Job if the 21st Century,‚Äù they primarily use the Linkedin recommender system as the tool that revolutionized the business. A product - not a report, not a stakeholder meeting, etc. 

When the decision scientist title existed, data science was still about predictive analytics. Decision scientists have effectively been swallowed by the data science brand as expectations of what data scientists should do shifted. This impacted the field turning data science into a primarily product geared position, to more of a consultant. This is what caused the huge expectation for data scientists to revolutionize business by looking at the data and uncovering hidden trends that would give business x a huge competitive advantage and blow the rest of the competition in the dust. That‚Äôs why companies use to be willing to pay so much money for a data scientist. We all know, for the vast majority of companies‚Ä¶ it didn‚Äôt work out that way. 

Before the ML Engineer title existed, data scientists were the ML guys. ML use to be the very core of data science and what differentiated them from traditional analysts. It was forward looking more than backward looking. Facebooks data science ‚Äòcore‚Äô team use to require research degrees in CS and/or Statistics to be in this core team. Other companies were less restrictive, but their core data scientists also researched and applied machine learning methodologies. It wasn‚Äôt until Lyft entered the business that core data scientists began rebranding themselves as machine learning engineers and data science became more focused on analysis than product.

In 2018 Lyft singlehandedly changed the data science landscape. One if Lyfts core problems when hiring analytical staff was that due to the insane hype around data science, everyone was calling themselves a data scientist whether or not they had the skills to understand and apply machine learning. Lyft noticed that when they rebranded their unpopular BI roles as data scientists, these class of people that knew nothing about predictive analysis but called themselves data scientists would apply for these roles at mass. (They got paid more too given the new title) And those were exactly the type of people that Lyft needed. You know excel and can make some visualizations on tableau? Great welcome to the data science team. Now the data science umbrella composed mainly of BI analysts, data analysts, core data scientists, and everything in between. I believe at this time the Chief Data Scientist of some tech company infamously changed his instagram handle to read ‚ÄòMachine Learning Engineer‚Äô to differentiate himself from the new trend of what data science was becoming. 

How did this happen? Well, we‚Äôve all been there or heard this story. You get onboarded as a data scientist at a company, the CEO has created a whole data science team, everyone can make a dashboard, everyone knows excel. There is no data engineering team, no product, no path forward. That is why the CEO hired so many data scientists anyway, to uncover his businesses future. Next month, and hundreds of thousands of dollars later the entire team is gutted, the CEO is fired, and the company is restarting their data practice from the bottom up starting with data engineers and software engineers. Can‚Äôt code? You‚Äôre not in. Can‚Äôt build products you‚Äôre not in. You‚Äôre on the data science team? Great, how are those KPIs looking this month? Expectations of what data science was supposed to be were way too high. Data Scientists were expected to be business magicians and lead companies next to business leaders. Some did, most didn‚Äôt. Now people have a more realistic perspective for the data science role as a non revolutionary analyst type role.

Who‚Äôs to blame and what‚Äôs the lesson? 
No one‚Äôs to blame, this is the hype cycle that has existed with every new thing in business. There‚Äôs a whole ‚ÄòGen AI‚Äô hype right now where everyone thinks AI is a chatbot. Maybe this is a cautionary tale to business leaders and aspiring data professionals to dig deeper beyond the hype so you‚Äôre not left disillusioned. 

TL:DR;
Data science teams were mainly product focused machine learning teams until Lyft changed the landscape and rebranded their BI Analysts as Data Scientists. This rebranding was good for Lyft, but left many smaller companies disillusioned with Data Scientists as they began hiring BI Analyst types with the expectations that data science will revolutionize their company to become industry leaders. Those who pioneered advancements in machine learning under the data scientist title have rebranded themselves as machine learning engineers to differentiate themselves. Now the data science role is a non extraordinary analyst type role. Be careful around hype cycles so you too are not left disillusioned."
1707214652.0,588,Anyone elses company executives losing their shit over GenAI?,"The company I work for (large company serving millions of end-users), appear to have completely lost their minds over GenAI. It started quite well. They were interested, I was in a good position as being able to advise them. The CEO got to know me. The executives were asking my advice and we were coming up with some cool genuine use cases that had legs. However, now they are just trying to shoehorn gen AI wherever they can for the sake of the investors. They are not making rational decisions anymore. They aren't even asking me about it anymore. Some exec wakes up one day and has a crazy misguided idea about sticking gen AI somewhere and then asking junior (non DS) devs to build it without DS input. All the while, traditional ML is actually making the company money, projects are going well, but getting ignored. Does this sound familiar? Do the execs get over it and go back to traditional ML eventually, or do they go crazy and start sacking traditional data scientists in favour of hiring prompt engineers?","I work for a consulting company and it is madness.

Managing Directors andcm managment consultants try to sell Gen AI capabilities although we don‚Äôt have anyone trained in it.

for them it is ‚Äújust build a model for next week as proof of concept‚Äù. this is all the hype these days, worst thing is that people who don‚Äôt understand the technology at all are the ones making decisions on it (tbf this has always been the case)

I personally would love to work more on these projects, but I believe if company wants to go in that direction they should understand that they need to train staff in this space first. People don‚Äôt understand how long it takes to become somewhat competent in any technical discipline","Yeah, this is the phase on the ‚Äúhype cycle‚Äù I like to call ‚Äúirrational exuberance‚Äù when execs get super excited about a technology they don‚Äôt understand and then try to use it to solve every problem like a magic hammer. It happened with a bunch of other technologies like deep learning, blockchain and the metaverse. My recommendation is to express your concerns for unqualified people to develop these tools and then if they don‚Äôt listen to you, get out of the blast radius when it eventually fails.","Our CEO asked me whom I can recommend to host a prompting training... I told him not to believe everything he reads and that prompt trainers are the newest form of social media grifters and I'll put a training together myself, plus linked him to the relevant parts of the Azure Generative AI documentation. 

Haven't heard from them since...."
1706125898.0,578,New grad's job hunt in for a Data Analyst role in Canada,,"About me: 

Double major in Applied Math + Economics, ~3.0 gpa, no internships, lots of relevant extracurriculars + tech club leadership positions with 3 projects in data analytics, offer from one of the big 5 banks in Canada in the Financial Intelligence Unit as a data analyst.

Watch out for positions that offer you ""training"", they'll ask money from you. üôÑ

Edit: Some more information based on the questions below: 

- I started applying around early December. I got a call for the role right after New Years.

- I mainly used Linkedin to search for roles like Junior Data Analyst, Business Analyst, Risk Analyst

- This market is very bad, usually a new grad with 0 YOE could get an interview that required 1-2 YOE, I heard absolutely nothing back from those positions

- My projects:

1. machine learning project that took used a very popular videogame API to fetch player data to be pipelined into a tensorflow neural network, to predict a winning team

2. EDA + customer segmentation analysis on dataset on an ecommerce data, where I used SQL to aggregate data and then used that dataset in R to perform analysis. Showed results on PowerBI. 

3. Fraud Detection on credit card data with kNN, improved classification with SMOTE technique. Displayed results and performance of the classification algorithm with Tableau.

Other tips: 

- try to pick up some really basic level of HTML + CSS, and make your own portfolio website with HTML templates and show off your projects. Presentation is half the battle. It doesn't need to look complicated, just make it nice and neat. I got my templates from here: https://html5up.net/

- Try to make a project that is an ""end to end"" solution for something. 

- show some flair in your projects by doing something that interests you (mine was videogames, project #1), it shows personality

- one thing that the hiring manager said that stuck with me: ""I'd rather hire someone passionate with personality than someone who  looks great on paper but has no teamwork skills or social skills (all other things being equal)""",[removed],Hows the compensation for the big5 banks for data analyst?
1711394964.0,554,Name & Shame: Carlyle Group Investment Data Science,"I think we're due for a name & shame! Sharing my experience in case it's helpful for future applicants.

# Company & Role

The Carlyle Group is a Private Equity mega-fund. They essentially buy and flip companies like a real estate investor buys and flips houses. They've recently (in the past few years) spun up a data science org. My understanding is that the responsibilities of this role would entail assisting the deal team in commercial due diligences of prospective investments, assisting in portfolio operations and consulting on advanced analytics for the portfolio companies, as well as company wide data science initiatives. My impression was that this role would not be very involved in deal sourcing.

# My Background

* FAANG Senior DS
* Worked in management consulting in the past - primarily as a data science consultant for Silicon Valley tech companies but also did a commercial due diligence project with our M&A practice as a DS consultant
* Ivy League masters in CS / Top 20 undergrad

# Application Process & Experience

* I first cold applied online
* After a short period of time I received an email from a Carlyle recruiter with a link to a 2 hour Hackerrank exam. I did not first receive any introductory call or even an introductory email - just an email with a URL to Hackerrank.
* I decided to take the exam. It consisted of:
   * One SQL (medium / window functions)
   * One Python (leetcode easy)
   * Discrete probability (e.g. probability of making a full house if you randomly draw 5 cards from a standard deck)
   * Domain specific data science questions (e.g. how would you apply data science to this private equity problem)
   * Overall I felt comfortable with all aspects of the exam and felt that it was well within my wheelhouse
* After completing the exam I sent a note to the recruiter. They scheduled a call with the ""senior recruiter"" for end of week
* The call with senior recruiter was fairly standard and covered the nature of the team, responsibilities of the role, and my background. I thought the call went well and was under the impression that I'd be moving forward in the process  (though I've learned never to take what recruiters say at face value)
* At the end of the call the senior recruiter asked if I had taken the Hackerrank exam yet. I was a bit surprised that they did not already know the answer to that question.
* After exactly one week of radio silence since the initial call, I emailed the first recruiter to let them know that I had seen some progress in my other searches (true) and asked if my application was still in consideration. I did not receive a response to this email.
* I waited one more week (two weeks since the initial call and about three weeks since I took the exam) and emailed the senior recruiter for a status update. I didn't receive a response to this email either but will edit this post if they ever do respond.

# Conclusion

* At this point I've concluded that I've been ghosted. I can only speculate as to why. I'm leaning towards them just being highly disorganized.
* For future applicants I strongly, strongly advise not taking their HackerRank exam unless you don't mind having your time wasted. I'm willing to bet nobody at Carlyle even looked at my test responses.

\*\*EDIT\*\*

It seems a lot of you think that ghosting is professionally acceptable. If you're investing your time, the bare minimum is a courtesy email to let you know you won't be moving forward in the process. That's actually table stakes. Apologies if you were expecting juicier drama!","This totally sucks and I think it‚Äôs a really sad reflection on the state of DS job searching that our immediate reaction to this is, ‚ÄúYou‚Äôre overreacting. This is the baseline for job searching now.‚Äù","We don't test at that level of rigor until the end of the process, not the beginning. And we are careful about promising detailed feedback and coaching on the test whether or not they are hired, because we know it's a real investment of time and effort.

This is awful. And no, ghosting after you've taken that kind of test AND interviewed is not acceptable behavior.","Shaming a company for ghosting an applicant?  I guess that's fine but I was hoping for something juicy and corrupt behind the scenes, not something that is super common in the industry."
1706307981.0,523,What is the dumbest thing you have seen in data science?,"What are the dumbest things that I have ever seen in data science is someone who created this elaborate Tableau dashboard that took months to create, tons of calculated fields and crazy logic, for a director who asked that the data scientist on the project then create a python script that will take pictures of the charts in the dashboard, and send them out weekly in an email. This was all automated. Like, I was shocked that anyone would be doing something so silly, and ridiculous. You have someone create an entire dashboard for months, and you can't even be bothered to look at it? You just want screenshots of it in your email, wasting tons of space, tons of query time, because you're too lazy to look at a freaking dashboard? 


What is the dumbest thing you guys have seen?","I had a coworker on a team I worked on come back with a 99% accurate model. It nearly went into production but I was curious and took a look at the code. 

Turns out he trained on the test data and ran it for 500 epocs. It was an overfit mess.","Company spent $2M on a vendor to solve a regression forcasting problem. Vendor couldn't solve the problem so they converted it into a classification problem of more than 2 or less than 2. It was always more than 2, you didn't need a model for that. Vendor said they got great accuracy and no one paying the bill undersood enough to question it.",Building elaborate dashboards only to have coworkers asking for excel exports.
1706151735.0,513,"798 applications later, I got a job.",,Was laid off mid-'23. 8yoe. It was rough. Turned down an offer I should have accepted. Accepted an offer and it was a shit show so I left knowing I had another offer. Really like where I'm at now.,Did your job start from referral or non-referral,What were the shitshows so at least we know to steer clear without getting yourself in trouble
1706913564.0,462,It's tough out there but sometimes you get lucky!,Been grinding LeetCode+LinkedIn for almost a month and it just paid off!,"Lol at ‚Äúscam?‚Äù 

Congrats!",can you share some details on the scams?,Great job! But it‚Äôs not only luck if you worked hard for it. You deserve it
1705945754.0,410,Does anyone know of any good Titanic datasets?,"I‚Äôve been looking for datasets related to the titanic, particularly whether certain passengers were more likely to survive or not. 

Anyone know of anything out there for this?","Sorry, best I can do is Iris flowers data set.","Sounds interesting! Way better than my job, which is trying to sort out these damned flowers according to their sepal widths.",This one dataset will make you irresistible to employers.
1705956605.0,383,I just realized i dont know python,"For a while I was thinking that i am fairly good at it.  I work as DS and the people I work with are not python masters too. This led me belive I am quite good at it.  I follow the standards and read design patterns as well as clean code.

Today i saw a job ad on Linkedin and decide to apply it.  They gave me 30 python questions (not algorithms) and i manage to do answer 2 of them.

My self perception shuttered and i feel like i am missing a lot.  I have couple of projects i am working on and therefore not much time for enjoying life.  How much i should sacrifice more ?  I know i can learn a lot if i want to . But I am gonna be 30 years old tomorrow and I dont know how much more i should grind.

I also miss a lot on data engineering and statistics. It is too much to learn.  But on the other hand if i quit my job i might not find a new one.

Edit: I added some questions here.  

First image is about finding the correct statement. Second image another question.

https://preview.redd.it/eutfjzpn72ec1.png?width=1246&format=png&auto=webp&s=d14c6c62be94899edeb04252cc025bf3a82e1472

https://preview.redd.it/9hbqoypn72ec1.png?width=1244&format=png&auto=webp&s=5b3233e3826f53eac0835598a02e245f7892eca4

&#x200B;","Programmer for 20 years.

1. I have used ALL of these before.
2. But I don't remember which objects return which objects and exactly which methods names they have
3. I dynamically re-acquaint myself with documentation as I need components -- often takes less than 30 seconds to find what I need.

Would probably be unfair to demand someone know all of the python standard library methods and return objects.

(I still think people should be able to talk about this stuff above in a pseudocode manner without knowing the right object names/methods.)","None of us know even 10% of the things that are useful for a data scientist. I so frequently feel like I am lacking in so many different departments. I used to do more software engineering oriented work and knew C# and Java pretty reliably, and these days I feel like the only programming I'm competent in is the slice of python that we use for statistical testing. 

It's good to be constantly learning new things and refreshing the old things. Try to spend a little bit of time doing that every day - not a lot, just 15-30 minutes. Maybe find a project that interests you that you can work on in your free time. But don't make yourself feel bad for not knowing something, and don't make any rash decisions because you don't know as much as some peer or other.","Do you have an example of some of the questions you missed? It is hard to say if you should be worried or if the interviewer just googled 'python software dev questions' and threw them all on the interview. 

You can get pretty damn far in data science without being a Python Wizard."
1709828162.0,376,I need to show how grateful I am to this sub,"Thanks you guys fpr every single book recommendation, for every single career advice.

I took your recommendations seriously, studied the books you told me to study, and studied other videos on my own, learning everything I can learn on my own.

Then I took the advice someone here told is to talk to someone internally in the data science team, turns out, they were impressed by the scope of the projects I worked on for a sales analyst and how I improved everything data-related in the department and the lead told me once I am ready (I still have a probability course to finish and recap hands on ML) and I will be up for a transfer.

I will be a junior DS in 5 or 6 months time after being an analyst for 2 years (I started when I was 20) and it's all you guys, so, thanks. 

Edit: here's everything:

I started when I was 18 years old, in something that I never knew it would be my gate to this job: a sales agent. Been so for a whole year. This gave me a lot of business context, how a manager leads people under him, and how his manager looks at his performance and understood something about the hierarchical behavior of companies. 
Then, I left the job after a year, now it's the pandemic, I spent it leqrning Excel and basic statistics, all on YouTube.

Moving forward to when I was 20, I had no idea a data analyst is even a title, and got a job as an accountant at a small workshop, with college going on, and I was studying business administration and statistics.
The job was never an accountant or have anything to do with accounting, my manager at the time was a very smart guy, working with pen and paper as his ledger, then I introduced Excel, he was all in for it, I started creating tables for our sales and inventory and customers and places we work in.

He started asking questions, you said last month we made 40K, how come we make 45 this month? I started digging into our data unknowingly doing analysis.

His brother was a regular visitor, I learned that he is the head of data at a big startup in our country, saw what I did, kept giving me tasks and I answer with Excel.

Then, he gave me a course that I highly recommend about Excel: power tools in Excel, you can find sources on YouTube for it a lot (power query, power pivot and data modeling). I started applying DAX, and here comes my first book [Dax Guide](https://www.google.com.eg/books/edition/_/dtr8oQEACAAJ?hl=en&sa=X&ved=2ahUKEwiQo4eZ8OKEAxUXgP0HHXfgDTIQ7_IDKAB6BAgPEAM).

Then I started my LinkedIn journey, showing Excel and powerBI dashboards and applying to jobs, in data analysis, really that's all you need, business context, some technical tools to help you dig into the data and answer questions.

Then, I started reading about data science, how statistics is important and how much I liked it in college, here goes the second book, [Naked Statistics](https://www.amazon.com/Naked-Statistics-Stripping-Dread-Data-ebook/dp/B007Q6XLF2).
Here I learned to think with stats a bit.

Then, I found that I lack implementation to a lot of concepts to statistics, people recommended python for me, here there were two sources for me to learn from, YouTube courses got me up and running into how to write simple code in python and understand the syntax.

Later, DataCamp had tracks, I finished the Data Analyst with python and another one data analyst with SQL. This helped me BIG time in knowing where to go next. 

Note: I was doing all of that while working and being in college.

The DataCamp course had great courses about statistics and probability and simulation. While also practicing SQL, I got really good with it.

Now, got a job as a junior sales ops analyst (my role now). I got lucky, working on real problems and practicing what I learn.

Then started moving back to books, but I lacked problem solving mindset, read these books: [Stop Guessing](https://www.amazon.com/Stop-Guessing-Behaviors-Problem-Solvers/dp/162656986X) and[Lean Analytics](https://leananalyticsbook.com/).

This helped me big time understand how my work affects the company. 

Now it's time to show your work to stakeholders, I read this book: [Storytelling with data](https://leananalyticsbook.com/).

It's time to go back to the details of my job, It was all querying on metabase, an open source BI tool.

I was responsible for giving agents retailers to visit, so, Every morning, we are supposed to apply filters on our data (last order date, last visit date and some other features ) and tell the agent, visit 20 of those retailers and go home. I was doing all of that in an automated fashion with power query, creating automated pipelines was my passion in Excel. All I had to do was give it an updated file from our database, refresh the pipeline, take the new file, dump it into our system.

They do visit 20 retailers, but the problem reached the tech team, the data was too much to handle, requiring us to give a smaller set of retailers for the agents, specifically 40 retailers.

But how do we guarantee they are close to each other? Here come my first interaction with adata scientist.

I did all what I did in Excel but in python using pandas and then reached the point where I don't know how to give clusters.

He took my jupyter notebook, gave it to us back with the solution to our problem, with something I was not familiar with at the time, Kmeans constrained.
Which took only longitude, latitude gave each agent his route of 40 retailers.

I started taking notes from his improvements to my code and asked him, what did you do?

He told my my code was fine, but you used a lot of custom functions on operations that can be vectorized, I asked for a book recommendation about vectorized operations in pandas here, the guys recommended this [Data Wrangling in python book](https://www.amazon.com/Data-Wrangling-Python-Tools-Easier/dp/1491948817).

After that book, I was obsessed with data automation in python using pandas and numpy only.

I got also obsessed with vectorizing any operation in our code base, read something pandas specific now: [Effective Pandas](https://www.amazon.com/Effective-Pandas-Patterns-Manipulation-Treading/dp/B09MYXXSFM).

Then, it was the part where he interacted with our system API.

Since all our company data scientists and swes have access to snowflake and live databases, we, analysts, had access to only metabase.

I saw this as an opportunity to get known!

I wrote two functions used by our entire company, ret_metabase and interact_with_google_sheets
The first one connects to the API endpoint and then takes your credintials and the makes a session ID and gets your card ID string response in json and I convert it to a dataframe. The second requires an Api key, thenenables tge user to do anything with a google sheet, remove data set with a dataframe get data asa dataframe append on data filter views really anything in one function.
How did I learn to do all of that? A course on youtube , just type API development in python amd a book about data structures, [Grokking Algorithms](https://www.amazon.com/Effective-Pandas-Patterns-Manipulation-Treading/dp/B09MYXXSFM). This helped big time in optimizing my code performance and writing cleaner code.

I got known and these functions are in the companies library now and people use it all the time. And I even left funny comments in the documentation and Everything.

The kmeans thing got me really interested in machine learning and here's the first book you guys recommended: [ISLR](https://www.statlearning.com/).

It was really hard for me at first because I had not been introduced properly to those three topics:
1- linear algebra
2- calculus
3- probability and statistics
I took [Jon Krohn's live lessions](https://github.com/jonkrohn/ML-foundations) it's free on YouTube.

But those three were later taken (started linear algebra in November 23).

So I struggled back then and here, another book was suggested: Hands-on ML.

I finished it and was really fucking hyped to apply the stuff I learned directly into my job, even without my manager permissions.

But that was not enough, I did not know what I should do to impact our compqny, what is data science?

I read this book: [Data science with business, what you need to know about DS](https://www.amazon.com/Data-Science-Business-Data-Analytic-Thinking/dp/1449361323)

First thing I dod after understanding what kmeans is, improved our routes clustering function by standerdizing the scales of the long, lat, giving it another column ( retailer rank) that rankstarts at the maximum value the longitude and decays linearly from 31 to 30 (longitude here is from 30 to 31), I used linspace and select in numpy here to give retailers ranks. This rank was business objective (give 31 toretailers with high conversion and then 30.9 to retailers with monotonically decreasing nmv to make them order back and so on...) Any other retailer takes a zero in his face. This helped in giving optimized distance to retailers we really need to visit.

This gave us a big boost in agents strike rate and overall performance.

Second, I applied xgboost, predicting who will place an order today if visited. Gave them the biggest rank.

Testing this was a must, so I learned about A/B testing, and some other great bootstrapping ideas here [Practical Statistics](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/) Book.

This pushed our strike rate from 40 to 73%.

Then, I really now see that I lack probability knowledge and maths knowledge to be a data scientist, so I read [Essential maths for DS](https://www.amazon.eg/-/en/Essential-Math-Data-Science-Fundamental/dp/1098102932).

Since my job was about sales operations, it was a necessary thing to automate discovering new sales areas and opportunity, previously, we used to draw polygons in areas we want to open, and then the agents are set there to wander and find retailers on their own.

I got an idea, how about I get all streets know in this area and make blocks in the intersections and then convert the coords to google maps link and give 50 daily sequential links to agents to discover areas in a more naturally sequential way? I used omnix API to get streets data and geopandas to make all other operations, I learned how to work with geopandas from their docs, really straightforward.

This project was big, applied everything I know about pandas and data structures and business knowledge to do it, and it's up and running now.

I got praised for it and the head of data was impressed with the result and decided to give me access to snowflake directly to limit requests on metabase as the data was big and then I scaled the project to all regions we operate in.

Then it was time to speak with the senior ds lead.

I showed him all I wrote here, he recommended I get a strong foundation in linear algebra and calculus and probability.

I got it, and now working on probability and statistics.

I then told him I am really into causal inference (rwcommended by someone in my previous post here) and regression analysis.

He said that's exactly what they need from the junior they want to hire, ""anyone can fit and predict nowdays"" he said, ""we need someone who can make an impact in all the stuff we don't have time for and teach him more cloud tools and maybe he gives us new ideas or show us new tools"" he elaborated.

Right now I am studying probability and statistics and then will study [Causal Inference](https://www.oreilly.com/library/view/causal-inference-in/9781098140243/).


I guess that's all, the most important thing is that you keep studying and never giving up, please, focus more on business context as it's overlooked.

I hope this was useful to you guys.","üòäCongrats! The summit of one mountain is the base of another, stay hard!¬†","Congrats! Could you share a summary of the books, resources, advice you found most helpful?","Okay okay spill the beans. We need a detailed roundup. What books and advice? 

And yes, love this sub!!"
1711634474.0,357,What is a Lead Junior Data Analyst?,,The blind leading the blind,You will have the responsibilities of a lead data scientist but you will be paid as a junior üòä,You can do easy projects on your own?
1704756059.0,328,Pre screening assessments are getting insane,"I am a data scientist in industry. I applied for a job of data scientist. 

I heard back regarding an assessment which is a word document from an executive assistant. The task is to automate anaysis for bullet masking cartilages. They ask to build an algorithm and share the package to them.

No data was provided, just 1 image as an example with little  explanation . They expect a full on model/solution to be developed in 2 weeks. 

Since when is this bullshit real, how is a data scientist expected to get the bullet cartilages of a 9mm handgun with processing and build an algorithm and deploy it in a package in the span of two weeks for a Job PRE-SCREENING.

Never in my life saw any pre screening this tough. This is a flat out project to do on the job.


Edit: i saw a lot of the comments from the people in the community. Thank you so much for sharing your stories. I am glad that I am not the only one that feels this way. 

Update: the company expects candidates to find google images for them mind it, do the forensic analysis and then train a model for them. Everything is to be handed to them as a package. Its even more grunt work where people basically collect data for them and build models. 

Update2: the hiring manager responds with saying this is a very basic straightforward task. Thats what the job does on a daily basis and is one of the easiest things a data scientist can do. Despite the overwhelming complexity and how tedious it is to manually do the thing. 
","My general thought, F that! I'm not doing that much for a chance to interview. 

I'm not saying this is what is happening; but some of these seem to be projects that they use as a pre-screening rather than pay for someone to do.

If you want me to do 30 minutes to display basic competency (i.e. eda, elt, basic ML interpretation) that's fine. Anything more and you can kiss my @ss. 

That's just my opinion though.",Free work maybe?,"Well as a SWE that moved into data analysis, this doesn't sound too far off from some interview tasks I have seen. I feel you completely. Its a completely unrealistic for someone working a full time job to perform these types of tasks in an interview.

And all honesty, any job I have applied for with this much work required is simple for me. Do what I can in a hour or two, and then note: I am a working professional and I do not have the time to commit to providing a complete work. I hope that my efforts here have provided insight into how I approach my work. I would love to meet to further discuss the work I have done here.

edit: additionally most of these jobs turned out to not be very worth it. So keep that in mind for how long you spend working."
1709143873.0,316,"If you are an X Analyst, what is your salary?","If you are an X Analyst, what is your salary?

Curious as to what the market looks like right now. Glassdoor, Indeed, Payscale and Salary.com all have a degree of variance, and it also depends on what *kind* of analyst you are.

I am:

-Risk Analyst L1, Financial Services industry

-Coming up to 2 YoE

-Total current comp $66,500 a year

-MCoL city, USA


Personally, very curious to hear from any Data, Risk and Credit Risk analysts out there!","Here is my history:

Data Analyst @ a university, 0 YOE, $40k

Marketing Analyst @ Fintech, 2 YOE, $90k. Laid off 

Data Analyst contract @ Google, 3 YOE, $152k. Made it the full 12 months.

Data Scientist contract @ Snap, 4 YOE, $156k. 5 months in

WTB FTE again ü´†","Risk Analytics Associate (senior analyst essentially)
- 5 YOE, 3 as data analyst , 1 as business analyst 
- 165k USD + 100k USD pre ipo (series D)  options 
- remote in Canada but pay in USD","Risk Data Analyst/Scientist 110k. About to hit 3 YOE

Electrical Engineering degree"
1707502557.0,313,Data science interviews are giant slogs still I see,"My department is cutting spend, so I decided to venture out and do some DS interviews and man I forgot how much trivia there is.

Like I have been doing this niche job within the DS world (causal inference in the financial space) for 5 years now, and quite successfully I might add. Why do I need to be able to identify a quadratic trend or explain the three gradient descent algorithims ad nauseum? Will I ever need to pull out probability and machine learning vocabulary to do my job? I‚Äôve been doing this (Causal Inference) work for which I‚Äôm interviewing for years, and these questions are not exemplary of this kind of work.

It‚Äôs just not reflective of the real world. We have copilot, ChatGPT, and google to work with everyday. Just man, not looking forward to re-reading all my grad school statistics and algerbra notes in prep for these over the top interviews.","The slog I found were the coding test, where a full day or twos worth of work was asked for. An instant no in response to that¬†","All the gotcha questions to see if you also just read the same passage the interviewer did from ""Hands-on Machine Learning with scikit-learn and Tensorflow"" are a complete waste of time and indicate that at least the interviewer and probably the whole organization are immature in their approach to DS. 
The only good way to conduct technical interviews that I have experienced is to present the interviewer with a dataset/problem-set and have them try to approach it in their own way. Then ask them why they did it the way they did it without looking for an exact vocab word.
I don't have to remember the word ""heteroskedacticity"" to notice it and talk coherently about how to handle it.",[deleted]
1706457085.0,298,"UPDATE #2: I built an app to make my job search a little more sane, and I thought others might like it too! No ads, no recruiter spam, etc.","Hey again everyone!

&#x200B;

We've made a lot of progress on [zen](https://zensearch.jobs/) in the past few months, so I'll drop a couple of the most important things / highlights about the app here:

* Zen is still a candidate / seeker-first job board. This means we have no ads, we have no promoted jobs from companies who are paying us, we have no recruiters, etc. The whole point of Zen is to help you find jobs quickly at companies you're interested in without any headaches.
* On that point, we'll send you emails notifying you when companies you care about post new jobs that match your preferences, so you don't need to continuously check their job boards.

&#x200B;

In the past few months, we've made some major changes! Many of them are [discussed in the changelog](https://zensearch.jobs/changelog):

1. [We now have a much more feature-complete way of matching you to relevant jobs](https://zensearch.jobs/jobs)
2. We've collected a ton of new jobs and companies, so we now have \~2,700 companies in our database and almost 100k open jobs!
3. We've overhauled the UX to make it less noisy and easier for you to find jobs you care about.
4. We also added [a feedback page](https://zensearch.jobs/feedback) to let you submit feedback about the app to us!

&#x200B;

I started building Zen when I was on the job hunt and realized it was harder than it should've been to just get notifications when a company I was interested in posted a job that was relevant to me. And we hope that this goal -- to cut out all the noise and make it easier for you to find great matches -- is valuable for everyone here :)  


Here are the original posts:   


* [https://www.reddit.com/r/datascience/comments/183562x/update\_i\_built\_an\_app\_to\_make\_my\_job\_search\_a/](https://www.reddit.com/r/datascience/comments/183562x/update_i_built_an_app_to_make_my_job_search_a/)
* [https://www.reddit.com/r/datascience/comments/17s5fyq/i\_built\_an\_app\_to\_make\_my\_job\_search\_a\_little/](https://www.reddit.com/r/datascience/comments/17s5fyq/i_built_an_app_to_make_my_job_search_a_little/)

&#x200B;

[And here's one more link to the app](https://zensearch.jobs)","Wow thank you!

Also, what‚Äôs the catch?¬†","Hi! Cool project, and a nice goal! It's exciting to see someone being able to ramp up a company/job database quickly!

Not so long ago, I used to work on Search/Matching/Relevance in one of the largest job search websites in the world. I could probably help with some discussions/ideas, feel free to reach out!",How do you get the jobs data? Scraping or APis?
1711181199.0,291,Scikit-learn Visualization Guide: Making Models Speak,"Use the Display API to replace complex Matplotlib code 

[ Scikit-learn Visualization Guide: Making Models Speak. ](https://preview.redd.it/pimy1i38a1qc1.png?width=896&format=png&auto=webp&s=f31492e48a5d39171c60d8e96ee8698c670327e7)

# Introduction

 In the journey of machine learning, explaining models with visualization is as important as training them. 

 A good chart can show us what a model is doing in an easy-to-understand way. Here's an example: 

[ Decision boundaries of two different generalization performances. ](https://preview.redd.it/3rrwu8rfa1qc1.png?width=863&format=png&auto=webp&s=9425bbe57deb98a854a40fdc0979149637078047)

 This graph makes it clear that for the same dataset, the model on the right is better at generalizing. 

 Most machine learning books prefer to use raw Matplotlib code for visualization, which leads to issues: 

1. You have to learn a lot about drawing with Matplotlib.
2. Plotting code fills up your notebook, making it hard to read.
3. Sometimes you need third-party libraries, which isn't ideal in business settings.

 Good news! Scikit-learn now offers Display classes that let us use methods like from\_estimator and from\_predictions to make drawing graphs for different situations much easier. 

 Curious? Let me show you these cool APIs. 

 

# Scikit-learn Display API Introduction

### Use utils.discovery.all_displays to find available APIs

 Scikit-learn (sklearn) always adds Display APIs in new releases, so it's key to know what's available in your version. 

 Sklearn's [utils.discovery.all\_displays](https://scikit-learn.org/stable/modules/generated/sklearn.utils.discovery.all_displays.html?ref=dataleadsfuture.com#sklearn.utils.discovery.all_displays) lets you see which classes you can use. 

    from sklearn.utils.discovery import all_displays
    
    displays = all_displays()
    displays

 For example, in my Scikit-learn 1.4.0, these classes are available: 

    [('CalibrationDisplay', sklearn.calibration.CalibrationDisplay),
     ('ConfusionMatrixDisplay',
      sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay),
     ('DecisionBoundaryDisplay',
      sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay),
     ('DetCurveDisplay', sklearn.metrics._plot.det_curve.DetCurveDisplay),
     ('LearningCurveDisplay', sklearn.model_selection._plot.LearningCurveDisplay),
     ('PartialDependenceDisplay',
      sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay),
     ('PrecisionRecallDisplay',
      sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay),
     ('PredictionErrorDisplay',
      sklearn.metrics._plot.regression.PredictionErrorDisplay),
     ('RocCurveDisplay', sklearn.metrics._plot.roc_curve.RocCurveDisplay),
     ('ValidationCurveDisplay',
      sklearn.model_selection._plot.ValidationCurveDisplay)]

### Using inspection.DecisionBoundaryDisplay for decision boundaries

 Since we mentioned it, let's start with decision boundaries. 

 If you use Matplotlib to draw them, it's a hassle: 

* Use np.linspace to set coordinate ranges;
* Use plt.meshgrid to calculate the grid;
* Use plt.contourf to draw the decision boundary fill;
* Then use plt.scatter to plot data points.

 Now, with  [inspection.DecisionBoundaryDispla](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html?ref=dataleadsfuture.com#sklearn-inspection-decisionboundarydisplay), you can simplify this process: 

    from sklearn.inspection import DecisionBoundaryDisplay
    from sklearn.datasets import load_iris
    from sklearn.svm import SVC
    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt
    
    iris = load_iris(as_frame=True)
    X = iris.data[['petal length (cm)', 'petal width (cm)']]
    y = iris.target
    
    
    svc_clf = make_pipeline(StandardScaler(), 
                            SVC(kernel='linear', C=1))
    svc_clf.fit(X, y)
    
    display = DecisionBoundaryDisplay.from_estimator(svc_clf, X, 
                                                     grid_resolution=1000,
                                                     xlabel=""Petal length (cm)"",
                                                     ylabel=""Petal width (cm)"")
    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, edgecolors='w')
    plt.title(""Decision Boundary"")
    plt.show()

 See the final effect in the figure: 

[ Use DecisionBoundaryDisplay to draw a triple classification model. ](https://preview.redd.it/501h2r27b1qc1.png?width=665&format=png&auto=webp&s=f3614719f48e74c6aabd0d0754c6b08b80f87d02)

 Remember, Display can only draw 2D, so make sure your data has only two features or reduced dimensions. 

### Using calibration.CalibrationDisplay for probability calibration

 To compare classification models, probability calibration curves show how confident models are in their predictions. 

 Note that  [CalibrationDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibrationDisplay.html?ref=dataleadsfuture.com#sklearn.calibration.CalibrationDisplay) uses the model's  predict\_proba. If you use a support vector machine, set probability to True: 

    from sklearn.calibration import CalibrationDisplay
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import make_classification
    from sklearn.ensemble import HistGradientBoostingClassifier
    
    X, y = make_classification(n_samples=1000,
                               n_classes=2, n_features=5,
                               random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                        test_size=0.3, random_state=42)
    proba_clf = make_pipeline(StandardScaler(), 
                              SVC(kernel=""rbf"", gamma=""auto"", 
                                  C=10, probability=True))
    proba_clf.fit(X_train, y_train)
    
    CalibrationDisplay.from_estimator(proba_clf, 
                                                X_test, y_test)
    
    hist_clf = HistGradientBoostingClassifier()
    hist_clf.fit(X_train, y_train)
    
    ax = plt.gca()
    CalibrationDisplay.from_estimator(hist_clf,
                                      X_test, y_test,
                                      ax=ax)
    plt.show()

[ Charts drawn by CalibrationDisplay. ](https://preview.redd.it/ovcp8hcgb1qc1.png?width=599&format=png&auto=webp&s=93515b81145abbb70424dcc69bf0cf8569ac4e3a)

### Using metrics.ConfusionMatrixDisplay for confusion matrices

 When assessing classification models and dealing with imbalanced data, we look at precision and recall. 

 These break down into TP, FP, TN, and FN ‚Äì a confusion matrix. 

 To draw one, use  [metrics.ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-confusionmatrixdisplay). It's well-known, so I'll skip the details. 

    from sklearn.datasets import fetch_openml
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import ConfusionMatrixDisplay
    
    digits = fetch_openml('mnist_784', version=1)
    X, y = digits.data, digits.target
    rf_clf = RandomForestClassifier(max_depth=5, random_state=42)
    rf_clf.fit(X, y)
    
    ConfusionMatrixDisplay.from_estimator(rf_clf, X, y)
    plt.show()

[ Charts drawn with ConfusionMatrixDisplay. ](https://preview.redd.it/tfbcdin8f1qc1.png?width=572&format=png&auto=webp&s=b1eaaf8e085e1cff8bb9c4bd971dade508010390)

### metrics.RocCurveDisplay and metrics.DetCurveDisplay

 These two are together because they're often used to evaluate side by side. 

 [RocCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html?ref=dataleadsfuture.com#sklearn.metrics.RocCurveDisplay) compares TPR and FPR for the model. 

 For binary classification, you want low FPR and high TPR, so the upper left corner is best. The Roc curve bends towards this corner. 

 Because the Roc curve stays near the upper left, leaving the lower right empty, it's hard to see model differences. 

 So, we also use [DetCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DetCurveDisplay.html?ref=dataleadsfuture.com#sklearn.metrics.DetCurveDisplay) to draw a Det curve with FNR and FPR. It uses more space, making it clearer than the Roc curve. 

 The perfect point for a Det curve is the lower left corner. 

    from sklearn.metrics import RocCurveDisplay
    from sklearn.metrics import DetCurveDisplay
    
    X, y = make_classification(n_samples=10_000, n_features=5,
                               n_classes=2, n_informative=2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                        test_size=0.3, random_state=42,
                                                        stratify=y)
    
    
    classifiers = {
        ""SVC"": make_pipeline(StandardScaler(), SVC(kernel=""linear"", C=0.1, random_state=42)),
        ""Random Forest"": RandomForestClassifier(max_depth=5, random_state=42)
    }
    
    fig, [ax_roc, ax_det] = plt.subplots(1, 2, figsize=(10, 4))
    for name, clf in classifiers.items():
        clf.fit(X_train, y_train)
        
        RocCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_roc, name=name)
        DetCurveDisplay.from_estimator(clf, X_test, y_test, ax=ax_det, name=name)

[ Comparison Chart of RocCurveDisplay and DetCurveDisplay. ](https://preview.redd.it/odavl54yh1qc1.png?width=884&format=png&auto=webp&s=6408e319b099ce12c3eff70e788ed0224558e099)

### Using metrics.PrecisionRecallDisplay to adjust thresholds

 With imbalanced data, you might want to shift recall and precision. 

* For email fraud, you want high precision.
* For disease screening, you want high recall to catch more cases.

 You can adjust the threshold, but what's the right amount? 

 Here, [metrics.PrecisionRecallDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-precisionrecalldisplay) can help. 

    from xgboost import XGBClassifier
    from sklearn.datasets import load_wine
    from sklearn.metrics import PrecisionRecallDisplay
    
    wine = load_wine()
    X, y = wine.data[wine.target<=1], wine.target[wine.target<=1]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                        stratify=y, random_state=42)
    
    xgb_clf = XGBClassifier()
    xgb_clf.fit(X_train, y_train)
    
    PrecisionRecallDisplay.from_estimator(xgb_clf, X_test, y_test)
    plt.show()

[ Charting xgboost model evaluation using PrecisionRecallDisplay.  ](https://preview.redd.it/8giy5tr8i1qc1.png?width=489&format=png&auto=webp&s=06b96373c182f515a655d6f4029f44982c87b05d)

 This shows that models following Scikit-learn's design can be drawn, like xgboost here. Handy, right? 

### Using metrics.PredictionErrorDisplay for regression models

 We've talked about classification, now let's talk about regression. 

 Scikit-learn's [metrics.PredictionErrorDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PredictionErrorDisplay.html?ref=dataleadsfuture.com#sklearn-metrics-predictionerrordisplay) helps assess regression models. 

    from sklearn.svm import SVR
    from sklearn.metrics import PredictionErrorDisplay
    
    rng = np.random.default_rng(42)
    X = rng.random(size=(200, 2)) * 10
    y = X[:, 0]**2 + 5 * X[:, 1] + 10 + rng.normal(loc=0.0, scale=0.1, size=(200,))
    
    reg = make_pipeline(StandardScaler(), SVR(kernel='linear', C=10))
    reg.fit(X, y)
    
    fig, axes = plt.subplots(1, 2, figsize=(8, 4))
    PredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[0], kind=""actual_vs_predicted"")
    PredictionErrorDisplay.from_estimator(reg, X, y, ax=axes[1], kind=""residual_vs_predicted"")
    plt.show()

[Two charts were drawn by PredictionErrorDisplay.](https://preview.redd.it/9uxuobuti1qc1.png?width=764&format=png&auto=webp&s=7db276b2a7d637cf53887e4cebe87b9a23593b9b)

 As shown, it can draw two kinds of graphs. The left shows predicted vs. actual values ‚Äì good for linear regression. 

 However, not all data is perfectly linear. For that, use the right graph. 

 It compares real vs. predicted differences, a residuals plot. 

 This plot's banana shape suggests our data might not fit linear regression. 

 Switching from a linear to an rbf kernel can help. 

    reg = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=10))

[ A visual demonstration of the improved model performance.  ](https://preview.redd.it/fsu1yqg0j1qc1.png?width=763&format=png&auto=webp&s=860447d0c8a1c0c38539f5fac16998ed928c8714)

 See, with rbf, the residual plot looks better. 

### Using model_selection.LearningCurveDisplay for learning curves

 After assessing performance, let's look at optimization with [LearningCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LearningCurveDisplay.html?ref=dataleadsfuture.com#sklearn.model_selection.LearningCurveDisplay).

  First up, learning curves ‚Äì how well the model generalizes with different training and testing data, and if it suffers from variance or bias. 

 As shown below, we compare a DecisionTreeClassifier and a GradientBoostingClassifier to see how they do as training data changes. 

    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.model_selection import LearningCurveDisplay
    
    X, y = make_classification(n_samples=1000, n_classes=2, n_features=10,
                               n_informative=2, n_redundant=0, n_repeated=0)
    
    tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)
    gb_clf = GradientBoostingClassifier(n_estimators=50, max_depth=3, tol=1e-3)
    
    train_sizes = np.linspace(0.4, 1.0, 10)
    fig, axes = plt.subplots(1, 2, figsize=(10, 4))
    LearningCurveDisplay.from_estimator(tree_clf, X, y,
                                        train_sizes=train_sizes,
                                        ax=axes[0],
                                        scoring='accuracy')
    axes[0].set_title('DecisionTreeClassifier')
    LearningCurveDisplay.from_estimator(gb_clf, X, y,
                                        train_sizes=train_sizes,
                                        ax=axes[1],
                                        scoring='accuracy')
    axes[1].set_title('GradientBoostingClassifier')
    plt.show()

[ Comparison of the learning curve of two different models. ](https://preview.redd.it/2ljyfl9aj1qc1.png?width=896&format=png&auto=webp&s=7d7af8bd8a4295c54f6f5c3ce41caf2fd2ddb0c1)

 The graph shows that although the tree-based GradientBoostingClassifier maintains good accuracy on the training data, its generalization capability on test data does not have a significant advantage over the DecisionTreeClassifier. 

### Using model_selection.ValidationCurveDisplay for visualizing parameter tuning

 So, for models that don't generalize well, you might try adjusting the model's regularization parameters to tweak its performance. 

 The traditional approach is to use tools like GridSearchCV or Optuna to tune the model, but these methods only give you the overall best-performing model and the tuning process is not very intuitive. 

 For scenarios where you want to adjust a specific parameter to test its effect on the model, I recommend using [model\_selection.ValidationCurveDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ValidationCurveDisplay.html?ref=dataleadsfuture.com#sklearn.model_selection.ValidationCurveDisplay) to visualize how the model performs as the parameter changes. 

    from sklearn.model_selection import ValidationCurveDisplay
    from sklearn.linear_model import LogisticRegression
    
    param_name, param_range = ""C"", np.logspace(-8, 3, 10)
    lr_clf = LogisticRegression()
    
    ValidationCurveDisplay.from_estimator(lr_clf, X, y,
                                          param_name=param_name,
                                          param_range=param_range,
                                          scoring='f1_weighted',
                                          cv=5, n_jobs=-1)
    plt.show()

[ Fine-tuning of model parameters plotted with ValidationCurveDisplay. ](https://preview.redd.it/5dcgo35jj1qc1.png?width=634&format=png&auto=webp&s=ed3d587e59d31902951ba0c9869ccd2bc39476d4)

&#x200B;

# Some regrets

 After trying out all these Displays, I must admit some regrets: 

 

* The biggest one is that most of these APIs lack detailed tutorials, which is probably why they're not well-known compared to Scikit-learn's thorough documentation.
* These APIs are scattered across various packages, making it hard to reference them from a single place.
* The code is still pretty basic. You often need to pair it with Matplotlib's APIs to get the job done. A typical example is DecisionBoundaryDisplay  
, where after plotting the decision boundary, you still need Matplotlib to plot the data distribution.
* They're hard to extend. Besides a few methods validating parameters, it's tough to simplify my model visualization process with tools or methods; I end up rewriting a lot.

 I hope these APIs get more attention, and as versions upgrade, visualization APIs become even easier to use. 

 

# Conclusion

 In the journey of machine learning, explaining models with visualization is as important as training them. 

 This article introduced various plotting APIs in the current version of scikit-learn. 

 With these APIs, you can simplify some Matplotlib code, ease your learning curve, and streamline your model evaluation process. 

 Due to length, I didn't expand on each API. If interested, you can check the [official documentation](https://scikit-learn.org/stable/visualizations.html?ref=dataleadsfuture.com) for more details. 

 Now it's your turn. What are your expectations for visualizing machine learning methods? Feel free to leave a comment and discuss. 

 This article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/scikit-learn-visualization-guide-making-models-speak/). ","Very nicely done. Thanks. Do you use seaborn? I really like it, as it also can plot interactive diagrams.","you can also use sklearn-evaluation, it has a bunch of extra things: [https://sklearn-evaluation.ploomber.io/en/latest/intro.html](https://sklearn-evaluation.ploomber.io/en/latest/intro.html)",Awesome post! You should also explore scikit-plot! Have really good graphs and they are easy to use.
1706819098.0,280,"I built an app to do my data science work faster, and I thought others here may like it too!",,"Hi everyone! I shared this in¬†r/ChatGPT¬†the other day and I thought people here may like it as well.

I built a tool that streamlines my data science work with Python notebooks using AI! I‚Äôve been having a bunch of fun tackling issues I‚Äôve typically had when working with notebooks, including:

* spending (too much) time in visualization library documentation to figure out how to color something a certain way
* finding repetitive boilerplate code to get started
* searching online to find new code to integrate into mine
* not knowing how to recover from errors

ChatGPT has been super helpful in tackling some of these but I wanted to integrate it directly into my workflow, and so I built this! I would love to hear the community‚Äôs thoughts üôè","How do you plan to differentiate from other notebook-type tools with AI assistance, i.e., Hex?","What is the difference between this and noteable, deepnote, datalore, notebooks in VS code and Pycharm with AI integrations ..?"
1709315022.0,271,What python data visualization package are you using in 2024?,"I've almost always used seaborn in the past 5 years as a data scientist. Looking to upgrade to something new/better to use!  
  
edit: looks like it's time to give plotly a shot!","Plotly, though it's never for anything user facing. Only EDA and internal purposes.",Plotly. People love interactive plots,Yup Plotly and Dash if you need a user interface
1710463723.0,272,A website for you to learn NLP,"Hi all,

I made a website that details NLP from beginning to end. It covers a lot of the foundational methods including primers on the usual stuff (LA, calc, etc.) all the way ""up to"" stuff like Transformers.

I know there's tons of resources already out there and you probably will get better explanations from YouTube videos and stuff but you could use this website as kind of a reference or maybe you could use it to clear something up that is confusing. I made it mostly for myself initially and some of the explanations later on are more my stream of consciousness than anything else but I figured I'd share anyway in case it is helpful for anyone. At worst, it at least is like an ordered walkthrough of NLP stuff

I'm sure there's tons of typos or just some things I wrote that I misunderstood so any comments or corrects are welcome, you can feel free to message me and I'll make the changes.

It's mostly just meant as a public resource and I'm not getting anything from this (don't mean for this to come across as self-promotion or anything) but yeah, have a look!

[www.nlpbegin.com](https://www.nlpbegin.com/)","On a quick skim, it seems really nice. I don't personally like learning from videos.

Bookmarking for later; will comment again if I end up going through it. Thanks for sharing and please reply here if you make other things like this in the future. It's a nice style",Very cool. I don‚Äôt use NLP now but I might in the future. Bookmarked for sure,"Did a quick navigation of the website and it looks really cool man. I haven't been working on NLP for the past two years, but wanted to visit the subject again right from the pre-neural networks era, and your website seems to be a great fit for me. Additionally, you've also covered neural nets and other pre-requisites. Great work, man! Will share this across my circle as well :)"
1709416904.0,247,[OC] Soon-to-be Political Science graduate‚Äôs two months job hunting for a Junior DS role in Colombia,"Transitioning from Social Sciences to Applied Sciences. ~1 YOE in DS internship roles. After my 5th interview my position was cancelled, but three weeks later they reached out again because they reopened it.

Both offers ended up being for a Junior DA position, but I'm really happy with the results. Thought my particular experience would be interesting to share. 
","Six interviews is WILD lol. Congrats, way to fight through and nail it down.",Congrats on getting a couple job offers. Happy for you!,Your hit rate is crazy high especially for a poli sci major
1705109532.0,239,What kind of Data Scientist is in demand for 2024?,"Looking for some insights into what skills should I learn in order to become a Data Scientist in demand. 
About me - I have almost 4 YOE in analytics. My skills are -
1. SQL
2. Python
3. Power BI and Tableau
4. Developing Machine learning (Supervised and Unsupervised models). But not in production, have a little to almost no experience in deploying them
5. Creating business presentations to explain model results and advising on marketing campaign strategy",Learn to deploy models to production.,[removed],"Hopefully roles involving bandits, active learning, and Bayesian optimization for experimentation"
1706280947.0,224,Companies should give employees a whole week (with no expected deliverables) dedicated to learning each year,"DS and analytics is a vast field and most employees will have gaps in their knowledge/skill that need to be filled. Each employee is unique so they are the ones who can best tell what they need to learn in order to (a) do better at work and also (b) grow in their career.

I feel many know what it is that they want to learn but cannot find time for it with their work load. And therefore, a dedicated learning week which can set expectations with all stakeholders that DS/analytics team is upgrading is must have. Maybe even employees can do knowledge sharing at end of week or start of next week. 

Company can eventually provide learning resources (courses, workshops, trainers etc.) but they shouldn't restrict employees on what they need to learn. It should at max be a discussion between employee and manager, where manager puts in suggestions but employee takes the final call. 

Please share your thoughts. Do you think such a thing would work?","Looking at your replies, I would actually disagree with you:

Companies shouldn't just give one week a year of free study for their employees - instead, companies should avoid using up 100% of their employee's time on known deliverables. 

>my work deliverables already take up anywhere between 60-90 hrs per week

That's your issue. It's not that you should keep working 60-90 hours a week and get one week a year to do some learning. It's that you should be working 35 hours a week and have 5 hours every week that you can choose what to do with.

Yes, I understand that some companies love this idea of squeezing as much work out of people as possible - and in my experience those are the companies that get stuck in ""doing things the same way they've always done them"", because no one has the bandwidth to rethink/redesign/reinvent any of their processes.",They don‚Äôt give. You take. I‚Äôve been taking for decades. It always pays back in me not doing the same old shit and elevating things.,"I think it is a red flag If work with DS and your boss don't give you freedom to learn on company time. DS is a vast area and there is always a ton to learn. Also, who expects people to now every statistical method from the top of the head, impossible. I produce new products all year long, and they always involve time for research."
1704388038.0,223,Where do the non-stupid people work?,"Edit: Thank you for all your insights. I have learned many people are totally fine with things breaking. In order for me to be a better coworker I need to accept and accommodate that. For example, if a server crashes and isn't fixed for 2 days I need to communicate that all our outputs may be MIA for two days and set that as the SLA.

Everyone I work with is a super smart moron. They‚Äôre super smart because they‚Äôre really good at engineering and can build really cool stuff. The problem is they don‚Äôt really care if their cool stuff actually works well. They don‚Äôt care about maintaining it or fixing issues quickly. They don‚Äôt care about providing status updates. Pretty basic stuff.

All my friends are experiencing the same issues I am facing. Their coworkers push code without testing. They approve untested code without verifying. They over engineer something because ‚Äùit‚Äôs cool‚Äù even if it runs like shit.

So I ask, where do the non-stupid people work?","To be fair, everyone I work with is a smart moron. In fact, the smarter I get I am starting to suspect that I, too, am a moron","People get promoted for building impressive stuff fast. People don't get promoted for fixing old bugs, pointing out mistakes or documenting code. I hate it too but that's how data science is run in many places.",[deleted]
1707251775.0,201,"How complex ARE your models in Industry, really? (Imposter Syndrome)","Perhaps some imposter syndrome, or perhaps not...**basically--how complex ARE your models, realistically, for industry purposes?** 

""Industry Purposes"" in the sense of answering business questions, such as:

* Build me a model that can predict whether a free user is going to convert to a paid user. (Prediction)
* Here's data from our experiment on Button A vs. Button B, which Button should we use? (Inference)
* Based on our data from clicks on our website, should we market towards Demographic A? (Inference)

I guess inherently I'm approaching this scenario from a prediction or inference perspective, and not from like a ""building for GenAI or Computer Vision"" perspective.

------------------

I know (and have experienced) that a lot of the work in Data Science is prepping and cleaning the data, but I always feel a little imposter syndrome when I spend the bulk of my time doing that, and then throw the data into a package that creates like a ""black-box"" Random Forest model that spits out the model we ultimately use or deploy.

Sure, along the way I spend time tweaking the model parameters (for a Random Forest example--tuning # of trees or depth) and checking my train/test splits, communicating with stakeholders, gaining more domain knowledge, etc., but ""creating the model"" once the data is cleaned to a reasonable degree is just loading things into a package and letting it do the rest. Feels a little too simple and cheap in some respects...especially for the salaries commanded as you go up the chain.

And since a lot of money is at stake based on the model performance, it's always a little nerve-wracking to hinge yourself on some black-box model that performed well on your train/test data and ""hope"" it generalizes to unseen data and makes the company some money.

Definitely much less stressful when it's just projects for academics or hypotheticals where there's no real-world repercussions...there's always that voice in the back of my head saying ""surely, something as simple as this needs to be improved for the company to deem it worth investing so much time/money/etc. into, right?""

------------------

Anyone else feel this way? Normal feeling--get used to it over time? Or is it that the more experience you gain, the bulk of ""what you are paid for"" isn't necessarily developing complex or novel algorithms for a business question, but rather how you communicate with stakeholders and deal with data-related issues, or similar stuff like that...?

------------------

**EDIT: Some good discussion about what types of models people use on a daily basis for work, but beyond saying ""I use Random Forest/XGBoost/etc."", do you incorporate more complexity besides the ""simple"" pipeline of: Clean Data -> Import into Package and do basic Train/Test + Hyperparameter Tuning + etc., -> Output Model for Use?**", 99% of models in my industry are linear regression,"It's not about complexity. It's about solving problems. 

My manager who is a senior director needs accuracy and attribution/explainability of variables that are being used. He doesn't care if it's a complicated LSTM or a basic SARIMA, Regression with lags or even a smoothing technique that gets the job done. 

This is for most DS roles unless you are talking about Research Scientists/MLEs whose main goal is to extract something specific from a recently published paper and use that in their models and be more upto date. Sure, that's great. Personally, I feel these folks lack business context and that's their tradeoff for being more complex/technical. Of course these folks get paid more as well due to the value attached to that skill set.","You‚Äôd be surprised how simple models combined with good domain knowledge can be. 

Which is why it‚Äôs interesting that things like earthgpt and timegpt are being hyped up despite nns not exactly being the go to or sota in a lot of problems-but I don‚Äôt think the practitioner is who they‚Äôre trying to sell this to (it‚Äôs probably the marketer)

Feels like prophet all over again.

Edit. I feel like perhaps I didn‚Äôt denote that I was speaking very generally. Not even in the prediction domain-but also that of inference."
1710955889.0,197,Can you have a successful career in this industry/field if you aren‚Äôt obsessed with it?,"I write this purely out of curiosity because I‚Äôm wondering if anyone else can relate to my situation.

I‚Äôm [M27] a Senior Data Analyst in the UK, with a Physics background and PhD. I‚Äôve generally always enjoyed learning and understand new concepts and ideas. 

After my PhD I left academia for industry - not because I didn‚Äôt like my subject, but because I wanted a permanent role away from the toxicity and pressure/risk of academia, with a 9-5 where I can switch off at the end of the day. Data seemed like a good starting point since I‚Äôd worked with it in my PhD.

On top of that, I didn‚Äôt want my subject to become my life - I enjoy far too many things outside of work to want to do it in my free time - I like sports, gaming, travelling, going for coffee, and all that lot. 

So, when I see people talking about their personal projects on GitHub it starts to make me wonder or question whether I‚Äôm in the right industry. Even when somebody creates a graph or produces some stats related to something I‚Äôm interested in, like Fantasy Premier League, it‚Äôs cool, but I have no desire to go and do that myself.

This is leading me to worry that long-term, I worry that I won‚Äôt be able to compete with the people who do genuinely live and breathe data 24/7. I‚Äôm just not ‚Äúobsessed‚Äù with it. I see it as a job. 

I also don‚Äôt want my lack of ‚Äúobsession‚Äù (if you want to call it that) be perceived for a lack of motivation or being someone who is lazy. I still want to progress over time, but above all I want to be comfortable.

Does anyone else feel the same way?
","I think one of the biggest mistakes is assuming passion makes you good at a job or that you have to pursue your passions professionally.

I like stats and data science, but it's a job, not my hobby. I proportion time during my work week to read up on papers and work on bigger ideas with unclear chances of success or even application. I take courses funded my job when it makes sense and something seems like it might pay off to learn.

Every so often I'll tackle a data problem in my free time for fun, but generally unless I am getting paid, I am not doing anything outside of my salaried or compensated hours.

I think it helps to have a general interest in stats, coding, data, and research. Or really to be the sort of person who enjoys answering questions via tools you built or at least tweaked. However, I don't think you have to spend every moment obsessing about data to have a nice career.","Yes you can. You will not be in the top 5% of applicants but I find that hobbies make you seem more relatable/well rounded in an interview. I am a hobbyist beekeeper and find a way to bring that up in almost every interview (usually in the context of being able to self teach), and the panelists are always super jazzed to talk about it in more detail.

Your job shouldn't consumer your entire life unless that is what your heart and mind are truly driven by.",Don‚Äôt overthink your way out of a good life.
1709364811.0,195,A Data science manager is just a manager,"As a data scientist from the days before it was a buzzword, I've had the hard journey from frustration over the lack of innovative projects at my company to ascending the ranks with the aim of being in the position to spearheading such initiatives. Initially, I thought the barrier was a lack of vision among decision-makers, but as I climbed the corporate ladder, I discovered the real challenge was not just creating groundbreaking projects, but ensuring their adoption within the company. Despite becoming proficient at the art of selling ideas and achieving some significant successes, the demands of management now consume all my time. I find myself mired in meetings, one-on-ones, and endless slide decks, leaving no space for the very innovation I sought to promote. This paradox highlighted a crucial lesson: having the power to initiate change doesn't guarantee the capacity to execute it, especially in a field where the talent for both data science and leadership is rare. The question then becomes: how do you find the balance?

Edit: To clarify, I do not feel the need to code or even solve develop the solution my self. I just want to be part of the internal innovation process and not be stuck maintaining a custom product a consultancy company got to build.","If you thought becoming a manager would give you more time to work on innovative data science projects, you played yourself.

The fact that you describe yourself as ""mired"" in the things managers are supposed to do, is a good indication you're not happy. Maybe that's the environment, unproductive meetings make me unhappy too, but maybe it's that you'd be happier as a lead/principal DS? There's no shame in that, that work is very important and very lucrative too.

I've been a manager and director of data teams for about 10 years now, and I love it. The greatest joy in my job is when my team are stuck on something and I can say the right thing to the right people, write the right memo, deliver the right presentation to get them moving. If that and the people management stuff doesn't make you happy, perhaps it's not for you?","As a manager, your job should be all about empowerment not really balance. You shouldn't need to be the smartest person in the room, you should just be hiring them. Then you should be cultivating them and setting them on a path to success even if that means growing beyond your team and company. Sure, it can be beneficial to the company to be able to jump in the race and help a fellow team member cross the finish line. However, a good manager can also find the right resources to help that team member cross if they cannot run themselves too.

I manage this way and when I switched to this approach, I have had most of my team come back to me after they left and tell me that without me, they wouldn't be where they are today. That's how I knew I was doing my job right as manager and it had nothing to do with me trying to balance the skills to do their job.","Management is about delivering through others, not delivering yourself. You are there to empower your reports, not be one. If you don't have the ability to do their job, you won't have credibility, but your purpose is first to enable them to do it to the best of their ability and second to improve that ability to the greatest possible extent. If you want to buy in to the system of exploitation, another function of management is to maximise the amount of work that you're getting out of your reports too, but I find that one to be much better when viewed as a consequence of empowerment and innovation rather than a primary aim.



Leadership is about being the reason those you're delivering through want to do it. Anyone at any level can practice it, it's not a management skill.


As you progressed up you probably should have been (made?) aware that management isn't about IC, there's a good reason the two tracks are distinct in sufficiently large organisations"
1711298148.0,199,Do you also wrap your data processing functions in classes?,"I work in a team of data scientists on time series forecasting pipelines, and I have the feeling that my colleagues overuse OOP paradigms. Let us say we have two dataframes, and we have a set of functions which calculates some deltas between them:

    def calculate_delta(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:
        delta = # some calculations incl. more functions
        return delta
    
    delta = calculate_delta(df1, df2)

What my coleagues usually do with this, that they wrap this function in a class, something like:

    class DeltaCalculatorProcessor:
        def __init__(self, df1: pd.DataFrame, df2: pd.DataFrame):
            self.__df1 = df1
            self.__df2 = df2
            self.__delta = pd.DataFrame()
    
        def calculate_delta(self) -> pd.DataFrame:
            ... # update self.__delta calculated from self.__df1 and self.__df2 using more class methods
            return self.__delta

And then they call it with

    dcp = DeltaCalculatorProcessor(df1, df2)
    delta = dcp.calculate_delta()

They always do this, even if they don't use this class more than once, so practically they just add yet another abstraction layer on the top of a set of functions, saying that ""this is how professional software developers do"", ""this is industrial best practice"" etc.

Do you also do this in your team? Maybe I have PTSD from having been a Java programmer before for ages, but I find the excessive use of classes for code structuring actually harder to maintain than just simply organizing the codes with functions, especially for data pipelines (where the input is a set of dataframes and the output is also a set of dataframes).

P.S. I wanted to keep my example short, so I haven't shown more smaller functions inside calculate_delta(). But the emphasis is not that they would wrap 1 single function in a class; but that they wrap a set of functions in a class without any further reasons (the wrapper class is not re-used, there is no internal state to maintain etc.). So the full app could be organized with pure functions, they just wrap the functions in ""Processor"" and ""Orchestrator"" classes, using one time classes for code organization.","Shows that DS can learn from some good SWE practice. Using functions (especially pure ones) is preferred considering you have less overhead. You only use classes if you have some kind of state to maintain. 

So yes, your colleagues are wrong.",It‚Äôs like data science with extra steps,"classes have a purpose over functions. if that purpose is not being utilized, it should not be used."
1711630718.0,201,New Causal ML book (free! online!),"Several big names at the intersection of ML and Causal inference, Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis have put out a new book (free and online) on using ML for causal inference. As you'd expect from the authors, there's a heavy emphasis on Double ML, but it seems like it covers a breadth of material. The best part? There's code in both Python and R.

Link: https://www.causalml-book.org/",This is the type of content I subscribe to this sub Reddit for.,I‚Äôm a novice at causal inference and ML but wanted to express what a time to be alive when stalwarts in the field just outright put things online for free. More power to the democratisation of education.,"Thank you for this.

A couple weeks back I asked a question on this sub about moving from a Predicitive framework to a casual one and I was more or less attacked for it. 

It good to see that actual professionals and experts are making good progress in this area."
1711811582.0,197,Where are the Junior Level Data Scientist Jobs?,"When I search for data type jobs on Indeed, I see analyst level jobs, and then  senior, lead, mostly director data scientist jobs. I hardly ever see Junior level jobs or even ""Data Scientist"" as a job title without a ""Director"" or ""Vice President"" attached. As you can imagine, this makes jumping from analyst to data scientist very difficult despite being qualified (MS stats, 7 years in various, increasingly senior analyst roles). Where are these roles?",The level of business understanding required for a lot of data science work kinda makes junior data scientist a difficult role to create. Someone with a few years of experience in an analyst role who has cursory experience building ML models is probably going to be more successful in a ‚Äústandard‚Äù data scientist role than a recent college grad who‚Äôs handy with ML but has very little experience in general.¬†,"Can I say something controversial?

Data Science is not a junior level job.  I know data science means a lot of different things depending on where you work, but generally it involves applying the scientific method + domain knowledge + taking into account business context.  Enough domain knowledge to make meaningful product recommendations is itself probably not something a junior candidate has, let alone good scientific skills (here, I'm not considering fitting models a scientific skill).

Analyst type jobs are the _de facto_  junior level IMHO, since this is where you're going to cut your teeth on core technologies and sharpen your domain expertise, and in all likelihood develop good scientific practices.

EDIT:  Addressing OP's question more directly

Tech -- where the majority of DS roles are -- is in a turbulent time right now. Data science is not a core need for a business like software engineering, so there are fewer ""Data Scientist"" roles (setting aside Senior for a moment).

When those roles are being hired for (and they are being hired for, I'm just not sure where you are looking), the roles are senior level (e.g. Senior DS, staff DS) because senior people have a better chance of being independent and not needing extensive hand holding. Because of my first point, companies currently can't afford to train junior people up, so they are hiring for reliable senior talent.

The roles exist, or at least have existed in the past, but market conditions are currently forcing orgs to reconsider who they hire and for what.","I agree with others that data science doesn‚Äôt have much room for junior roles. Usually only see junior DS at bigger / data-focused companies that have a business case for training up DS (massive tech company or very specialized domain w need to upskill talent). scientist very similar to senior analyst‚Äî unfortunately it means there aren‚Äôt many junior DS roles BUT fortunately, it means there is a clear (ish) path from analyst to DS. I would encourage you to look for an analyst role that involves doing some DS type of stuff, at a company you like, with room for advancement, establish yourself and then make the case for a transition to DS once you‚Äôve proven your value. If you land an analyst role and convince your employer to give you a DS title, you‚Äôve essentially ‚Äúmade it‚Äù and are forever after a data scientist, and in my experience this is so much easier than finding a DS role from scratch¬†"
1711452068.0,193,"For the first time, I have seen a job post appreciating having Coursera certificates.",,Is this good or bad?,"Sun shines on a something something every once in a while. I like talking to candidates about how they've broadened/deepened their horizons with these courses during later interviews but I can't use it as any kind of instrument early in the application process (they are too easy and there's no guarantee the candidate learned anything from them, really).

Credentialing is all over the place. Only reason I even care about a 4-8 year degree credentials is they tend to let me cut the stack of candidates down by 50-75%. I didn't do that before I started getting 1,000+ applications for every open position (post-pandemic).",This subreddit has an extremely skewed idea of how job postings are written. HR works with the manager of the team that needs a hire to put together a job description and list of qualifications. The postings you see reflect the personal preferences of that particular manager. They are made by actual people.
1706201329.0,187,I got rejected by Toward Datascience,"I have worked on several forecasting projects in the past few months, and I decided to write a blog to share my learnings and insights with data analysts and junior data scientists. After writing the blog, I submitted it to TDS. They rejected it, stating that 

    'the overall flow of the post was too disjointed and the approach to the topic was somewhat too high-level and not actionable/concrete enough.' 

I don't blame them for this feedback, and I've done some editing to make the article smoother. Has the article improved? Anything I should add to the article? I hope to turn this around and win back on TDS. Any advise will be helpful.  


I've post it here: [https://acho.io/blogs/why-i-perfer-tree-models](https://acho.io/blogs/why-i-perfer-tree-models)

&#x200B;","Do you have an editor - or someone who can act as an editor? It doesn't need to be someone with technical knowledge, but someone who does a lot of reading? (I use my mom).

(I don't intend to sound harsh here, I'm trying to be supportive).

""Just as weather forecasting is valuable in our daily lives, time-series forecasting plays an important role in business decisions."" Huh? Seems like a different thing, and it's kind of irrelevant. Someone has already decided to read the article, so they think it's worth knowing. You don't need to persuade them.

""Today, massive data is gathered through automations and systems every day, making time-series data increasingly prevalent and accessible.""  I don't know what automatons and systems are. Again, this is known. What about something like ""Organizations collect large amounts of data on a daily basis, which can be mined for insights and information.""

""Consequently, the ability to learn from these time-series data and generate accurate forecasts for the future is becoming ever more essential."" I don't see how that follows (is consequent). It's more essential? Essential is binary - it's essential or it's not. Maybe it's more useful, but why is it more useful than it would have been in the past? Presumably it's equally useful? But now we can do it? 

The article is taking too long to get to the point. 

I encourage you to keep writing. Pretty much the only way to get better at writing is to do more of it. There are lots of ways to practice writing (and perhaps gain a reputation, which might be useful, or you can point people towards).  Blogs, Reddit, CrossValidated, etc can all be good practice and sources of feedback - try writing something wrong on CrossValidated and see what happens. :)","I think you have a bit of mix of styles problem - you start with very trivial stuff that forecasting is important then go to trees and conclude there is a lot more to be taken into consideration.

 If I want trees, I do not want stuff about what is business forecasting, and vice versa.

 You definitely have something to say but it is deeply inside your writing, so the problem is kind of lack of focus - what is the value to the reader, noise/signal ratio in you text. Maybe make it shorter but to the point or split in two pieces.","Don't feel bad. I use to write for TDS (30+ published articles) and the new chief editor is awful (he has no technical background and even worse is very arrogant). That said as others point out your article does have several run-on sentences, is hard to follow at times, and repeats itself. It also has a bit too much business jargon for my liking. If you want to DM me I could give more detailed feedback.

I would honestly avoid publishing to TDS though and I say that as a prior author. They have gone down a road of pure clickbait and seem to now reject higher quality pieces they think might be too technical for their audience. It used to be they would just accept everything but now there is a lot more curation, however it is curation of clickbait."
1708458191.0,170,"""Prepare to be replaced by a data engineer""","I remember a tweet from a ML Head from a company of my country that was along the lines of ""If you are a Data Scientist and if in your daily preparation you dont include business readings then prepare to be replaced by a data engineer"".   What do you think about this statement?",I have no idea what this statement even means tbh,"Not saying data scientist is a higher skilled position than data engineer, but they're two completely different skillsets. Unless you can fully automate the tasks a data scientist does into an API call or a function, I doubt a data engineer would be able to do effective data science work. 

I'm guessing the ML Head has no idea what these two roles are and just think engineers are more practical than scientists. 

Not to question this person's credentials, but there are a lot of higher ups in the data field who shouldn't really be there, and only got there by being lucky, playing politics, and BSing their achievements. Okay maybe I am questioning this person's credentials.","Of course, I agree with others, those two positions are not comparable. But are connected.



What I am seeing in the last two years are more job openings for data engineers and less for data scientists.



One idea is that companies tried with data scientists, concluded that their data is not ready and that they need data engineers before hiring more DSs.



At least that's what I see in my company."
1707028017.0,173,Visualizing What Batch Normalization Is and Its Advantages," Optimizing your neural network training with Batch Normalization 

[Visualizing What Batch Normalization Is and Its Advantages](https://preview.redd.it/dah6fe3kgigc1.png?width=999&format=png&auto=webp&s=a0f8a08ae05cf2989d109ae274b6f65e5cc83002)

# Introduction

 Have you, when conducting deep learning projects, ever encountered a situation where the more layers your neural network has, the slower the training becomes? 

 If your answer is YES, then congratulations, it's time for you to consider using batch normalization now. 

# What is Batch Normalization?

 As the name suggests, batch normalization is a technique where batched training data, after activation in the current layer and before moving to the next layer, is standardized. Here's how it works: 

1. *The entire dataset is randomly divided into N batches without replacement, each with a mini\_batch size, for the training.*
2. *For the i-th batch, standardize the data distribution within the batch using the formula:* *(Xi - Xmean) / Xstd**.*
3. *Scale and shift the standardized data with* *Œ≥Xi + Œ≤* *to allow the neural network to undo the effects of standardization if needed.*

 The steps seem simple, don't they? So, what are the advantages of batch normalization? 

# Advantages of Batch Normalization

### Speeds up model convergence

 Neural networks commonly adjust parameters using gradient descent. If the cost function is smooth and has only one lowest point, the parameters will converge quickly along the gradient. 

 But if there's a significant variance in the data distribution across nodes, the cost function becomes less like a pit bottom and more like a valley, making the convergence of the gradient exceptionally slow. 

 Confused? No worries, let's explain this situation with a visual: 

 First, prepare a virtual dataset with only two features, where the distribution of features is vastly different, along with a target function: 

    rng = np.random.default_rng(42)
    
    A = rng.uniform(1, 10, 100)
    B = rng.uniform(1, 200, 100)
    
    y = 2*A + 3*B + rng.normal(size=100) * 0.1  # with a little bias

 Then, with the help of GPT, we use matplot3d to visualize the gradient descent situation before data standardization: 

[ Visualization of cost functions without standardization of data.  ](https://preview.redd.it/s7zorqm9higc1.png?width=537&format=png&auto=webp&s=f92388f9e3384eee96ce2585b7bc70f25392cee9)

 Notice anything? Because one feature's span is too large, the function's gradient is stretched long in the direction of this feature, creating a valley. 

 Now, for the gradient to reach the bottom of the cost function, it has to go through many more iterations. 

 But what if we standardize the two features first? 

    def normalize(X):
        mean = np.mean(X)
        std = np.std(X)
        return (X - mean)/std
    
    A = normalize(A)
    B = normalize(B)

 Let's look at the cost function after data standardization: 

[ Visualization of standardized cost functions for data. ](https://preview.redd.it/5ou0awyehigc1.png?width=618&format=png&auto=webp&s=72c3f4dd14b78d52f0fe1396a0c131d19e0f6aea)

 Clearly, the function turns into the shape of a bowl. The gradient simply needs to descend along the slope to reach the bottom. Isn't that much faster? 

### Slows down the problem of gradient vanishing

 The graph we just used has already demonstrated this advantage, but let's take a closer look. 

 Remember this function? 

[ Visualization of sigmoid function.  ](https://preview.redd.it/jl3cyhojhigc1.png?width=499&format=png&auto=webp&s=2c2e2047317d99d89b9232b37fa28652eb10f7bd)

 Yes, that's the sigmoid function, which many neural networks use as an activation function. 

 Looking closely at the sigmoid function, we find that the slope is steepest between -2 and 2. 

[ The slope of the sigmoid function is steepest between -2 and 2. ](https://preview.redd.it/j6l02trmhigc1.png?width=499&format=png&auto=webp&s=504efbe5562e9b477a4c48e943011dca7bab5fa7)

 If we reduce the standardized data to a straight line, we'll find that these data are distributed exactly within the steepest slope of the sigmoid. At this point, we can consider the gradient to be descending the fastest. 

[ The normalized data will be distributed in the steepest interval of the sigmoid function. ](https://preview.redd.it/kk0mrflphigc1.png?width=499&format=png&auto=webp&s=43d1dbc9b1a63952a41730146bfd624db97b2b4d)

 However, as the network goes deeper, the activated data will drift layer by layer (Internal Covariate Shift), and a large amount of data will be distributed away from the zero point, where the slope gradually flattens. 

[ The distribution of data is progressively shifted within the neural network. ](https://preview.redd.it/6y088bkthigc1.png?width=499&format=png&auto=webp&s=69ebded75578b8a0daad7874d3803b926db74363)

 At this point, the gradient descent becomes slower and slower, which is why with more neural network layers, the convergence becomes slower. 

 If we standardize the data of the mini\_batch again after each layer's activation, the data for the current layer will return to the steeper slope area, and the problem of gradient vanishing can be greatly alleviated. 

[ The renormalized data return to the region with the steepest slope.  ](https://preview.redd.it/pmkgqpuwhigc1.png?width=499&format=png&auto=webp&s=84ce3916984707d86f314a3dc458f403b0584999)

### Has a regularizing effect

 If we don't batch the training and standardize the entire dataset directly, the data distribution would look like the following: 

[ Distribution after normalizing the entire data set. ](https://preview.redd.it/a0nuru70iigc1.png?width=484&format=png&auto=webp&s=5bd763b5d450599cb6527089e01b0a1735aac4cb)

 However since we divide the data into several batches and standardize the data according to the distribution within each batch, the data distribution will be slightly different. 

[ Distribution of data sets after normalization by batch. ](https://preview.redd.it/of8wlwc2iigc1.png?width=484&format=png&auto=webp&s=16ad7cf81a7c08512399e7681ff3bce8568d11e7)

 You can see that the data distribution has some minor noise, similar to the noise introduced by Dropout, thus providing a certain level of regularization for the neural network. 

## Conclusion

 Batch normalization is a technique that standardizes the data from different batches to accelerate the training of neural networks. It has the following advantages: 

* Speeds up model convergence.
* Slows down the problem of gradient vanishing.
* Has a regularizing effect.

 Have you learned something new? 

 Now it's your turn. What other techniques do you know that optimize neural network performance? Feel free to leave a comment and discuss. 

 This article was originally published on my personal blog [Data Leads Future](https://www.dataleadsfuture.com/visualizing-what-batch-normalization-is-and-its-advantages/). ",Its always the advantages which are being talked about. I think its rathar important to talk about disadvantages and cases where it shouldnt be used.,OP this is really well done and simplifies the concept for me as a beginner to understand too. Thank you! Looking forward to more,What visualization lib is used here ?
1707863937.0,165,Would you agree? Focusing on mastering math is the best RoI for long-term satisfaction,"First of all, this is from the perspective of an analyst who is more on the business side, so let me know if I'm completely stupid.

Why I'm writing this - I think many people underestimate the basic ""boring"" math and they just go right to how neural networks function or how to use logistic regression

Algorithms keep changing, libraries keep changing, domain related knowledge will (partially) change as your economy sector evolves and you'll pick it up as you go anyway...

Even whatever university degree you pick is kind of arbitrary, some of them might make learning math easier for you, but you can always pick it up yourself - even if you study something seemingly unrelated, if you're smart enough for data science you can self-study math

If you're worried about long-term job prospects and satisfaction, it seems to me you should focus on making your main goal to master all possible areas of math. Even the ones directly unrelated to your work. Because data science (and tech in general) is a lifelong study and you will keep having to learn new stuff all the time.

But if you know the fundamental math behind it all, it will make it much easier to learn new algorithms for example. It will also be easier to pick up the logic behind certain principles within your domain, as you'll get better intuition. Part of this should be learning logic (whether you count this as math or philosophy is up to debate).

I am just thinking out loud and kind of looking for confirmation bias, because I've been learning all the juicy ML algorithms and libraries, programming languages etc in the past few years. And I'm thinking I should have just focused on getting better at statistics, probability, combinatorics, discrete math in general... linear algebra... calculus... hell, even if you go all the way back to elementary school or high school, there are surely some topics you forgot and they might be useful to re-learn (like some stuff from geometry that you NEVER used but it could be the missing piece from understanding some stuff you're working on now).

Because all the stuff I learned a few years ago is already obsolete anyway. But math is unchanged for hundreds and thousands of years. And still useful.

So recently I've more shifted to the theoretical side of things. And it's made me happier with problem solving and I have less impostor syndrome. All kinds of different word problems are good practice especially.

tldr: Instead of learning 50 ways to do similar things, learning the underlying math - not superficially, but all the way to the fundamentals, even all the way to elementary school if you forgot something - should be better for long-term.","Best ROI for long-term satisfaction is strengthening your relationships with family, friends, and the local community",">master all possible areas of math  

lol","Math certainly has a pretty good ROI and returns don‚Äôt really diminish as you learn more and more. However, after a certain point it‚Äôs probably better to focus on applied experience. It can‚Äôt hurt to understand these things at a deep level, but none of that matters if you can‚Äôt actually finish a project. I‚Äôd take someone with mediocre understanding of the algorithms but decent experience over a math wiz without experience."
1711925235.0,160,First job out of undergrad is really boring,"Hey all, im a fresh grad with a background with applied math and econ. I got a job really quickly after graduation as a data analyst at a large bank in my country (anti money laundering & compliance), but the actual responsibility of the role is more like a data entry position with excel. As you can imagine, it‚Äôs painfully dull and low paying aside from the advantage of good LSB (9-5). I‚Äôve been working on a way to automate my work with python scripts, but aside from this there is really not much to add to my resume. 

My overall goal is to move to a backoffice positon in risk/investment research unit in my bank where they do something more quantitative like analytics, modelling and statistical analysis. What else could I be doing to get there in the future? 

","This is common. Ten years ago you probably wouldn‚Äôt have been hired into a data analytics position until you had 5+ years of business experience under your belt. It‚Äôs not really an entry level job. Success in the field usually comes down to deep business knowledge, which takes time to acquire.

The explosion of data science popularity has created a lot of these junior positions that are being filled with maths grads straight out of university. But you don‚Äôt know enough about anything to be useful yet, so you will be kept doing fairly menial work until you learn more about the business.

Long term, don‚Äôt expect to use a ton of your maths degree. Every grad has one these days and there aren‚Äôt enough jobs with really fancy modelling to employ them all. Most of your job will be applying relatively simple models judiciously based on a deep understanding of the business need.",Completely normal.,"I honestly wish my job was boring. 

I dread Sundays because it's reminder that I have to head into another week of constant pings and a plethora of data analyses with everything on fire for no reason other than just because.

I'd switch with you any day. Be happy, and stay boring."
1711620007.0,155,Top Cities in the US for Data Scientists in terms of Salary vs Cost of Living,"We analyzed 20,000 US Data Science job postings from June 2024 - Jan 2024 with quoted salaries: computed median salaries by City, and compared them to the cost of living.

Source: [Data Scientists Salary article](https://jobs-in-data.com/salary/data-scientist-salary)

Here is the Top 10:

&#x200B;

https://preview.redd.it/jigjbhivs1rc1.png?width=1643&format=png&auto=webp&s=de294a1e3b4fdf46cbf30cfa64274aa3ae19a0dc

Here is the full ranking:

|Rank|City|Annual Salary|Annual Cost of Living|Annual Savings|N job offers|
|:-|:-|:-|:-|:-|:-|
|1|[Santa Clara](https://jobs-in-data.com/c-santa%20clara)|207125|39408|167717|537|
|2|[South San Francisco](https://jobs-in-data.com/c-south%20san%20francisco)|198625|37836|160789|95|
|3|[Palo Alto](https://jobs-in-data.com/c-palo%20alto)|182250|42012|140238|74|
|4|[Sunnyvale](https://jobs-in-data.com/c-sunnyvale)|175500|39312|136188|185|
|5|[San Jose](https://jobs-in-data.com/c-san%20jose)|165350|42024|123326|376|
|6|[San Bruno](https://jobs-in-data.com/c-san%20bruno)|160000|37776|122224|92|
|7|[Redwood City](https://jobs-in-data.com/c-redwood%20city)|160000|40308|119692|51|
|8|[Hillsboro](https://jobs-in-data.com/c-hillsboro)|141000|26448|114552|54|
|9|[Pleasanton](https://jobs-in-data.com/c-pleasanton)|154250|43404|110846|72|
|10|[Bentonville](https://jobs-in-data.com/c-bentonville)|135000|26184|108816|41|
|11|[San Francisco](https://jobs-in-data.com/c-san%20francisco)|153550|44748|108802|1034|
|12|[Birmingham](https://jobs-in-data.com/c-birmingham)|130000|22428|107572|78|
|13|[Alameda](https://jobs-in-data.com/c-alameda)|147500|40056|107444|48|
|14|[Seattle](https://jobs-in-data.com/c-seattle)|142500|35688|106812|446|
|15|[Milwaukee](https://jobs-in-data.com/c-milwaukee)|130815|24792|106023|47|
|16|[Rahway](https://jobs-in-data.com/c-rahway)|138500|32484|106016|116|
|17|[Cambridge](https://jobs-in-data.com/c-cambridge)|150110|45528|104582|48|
|18|[Livermore](https://jobs-in-data.com/c-livermore)|140280|36216|104064|228|
|19|[Princeton](https://jobs-in-data.com/c-princeton)|135000|31284|103716|67|
|20|[Austin](https://jobs-in-data.com/c-austin)|128800|26088|102712|369|
|21|[Columbia](https://jobs-in-data.com/c-columbia)|123188|21816|101372|97|
|22|[Annapolis Junction](https://jobs-in-data.com/c-annapolis%20junction)|133900|34128|99772|165|
|23|[Arlington](https://jobs-in-data.com/c-arlington)|118522|21684|96838|476|
|24|[Bellevue](https://jobs-in-data.com/c-bellevue)|137675|41724|95951|98|
|25|[Plano](https://jobs-in-data.com/c-plano)|125930|30528|95402|75|
|26|[Herndon](https://jobs-in-data.com/c-herndon)|125350|30180|95170|88|
|27|[Ann Arbor](https://jobs-in-data.com/c-ann%20arbor)|120000|25500|94500|64|
|28|[Folsom](https://jobs-in-data.com/c-folsom)|126000|31668|94332|69|
|29|[Atlanta](https://jobs-in-data.com/c-atlanta)|125968|31776|94192|384|
|30|[Charlotte](https://jobs-in-data.com/c-charlotte)|125930|32700|93230|182|
|31|[Bethesda](https://jobs-in-data.com/c-bethesda)|125000|32220|92780|251|
|32|[Irving](https://jobs-in-data.com/c-irving)|116500|23772|92728|293|
|33|[Durham](https://jobs-in-data.com/c-durham)|117500|24900|92600|43|
|34|[Huntsville](https://jobs-in-data.com/c-huntsville)|112000|20112|91888|134|
|35|[Dallas](https://jobs-in-data.com/c-dallas)|121445|29880|91565|351|
|36|[Houston](https://jobs-in-data.com/c-houston)|117500|26508|90992|135|
|37|[O'Fallon](https://jobs-in-data.com/c-o'fallon)|112000|24480|87520|103|
|38|[Phoenix](https://jobs-in-data.com/c-phoenix)|114500|28656|85844|121|
|39|[Boulder](https://jobs-in-data.com/c-boulder)|113725|29268|84457|42|
|40|[Jersey City](https://jobs-in-data.com/c-jersey%20city)|121000|36852|84148|141|
|41|[Hampton](https://jobs-in-data.com/c-hampton)|107250|23916|83334|45|
|42|[Fort Meade](https://jobs-in-data.com/c-fort%20meade)|126800|44676|82124|165|
|43|[Newport Beach](https://jobs-in-data.com/c-newport%20beach)|127900|46884|81016|67|
|44|[Harrison](https://jobs-in-data.com/c-harrison)|113000|33072|79928|51|
|45|[Minneapolis](https://jobs-in-data.com/c-minneapolis)|107000|27144|79856|199|
|46|[Greenwood Village](https://jobs-in-data.com/c-greenwood%20village)|103850|24264|79586|68|
|47|[Los Angeles](https://jobs-in-data.com/c-los%20angeles)|117500|37980|79520|411|
|48|[Rockville](https://jobs-in-data.com/c-rockville)|107450|28032|79418|52|
|49|[Frederick](https://jobs-in-data.com/c-frederick)|107250|27876|79374|43|
|50|[Plymouth](https://jobs-in-data.com/c-plymouth)|107000|27972|79028|40|
|51|[Cincinnati](https://jobs-in-data.com/c-cincinnati)|100000|21144|78856|48|
|52|[Santa Monica](https://jobs-in-data.com/c-santa%20monica)|121575|42804|78771|71|
|53|[Springfield](https://jobs-in-data.com/c-springfield)|95700|17568|78132|130|
|54|[Portland](https://jobs-in-data.com/c-portland)|108300|31152|77148|155|
|55|[Chantilly](https://jobs-in-data.com/c-chantilly)|133900|56940|76960|150|
|56|[Anaheim](https://jobs-in-data.com/c-anaheim)|110834|34140|76694|60|
|57|[Colorado Springs](https://jobs-in-data.com/c-colorado%20springs)|104475|27840|76635|243|
|58|[Ashburn](https://jobs-in-data.com/c-ashburn)|111000|34476|76524|54|
|59|[Boston](https://jobs-in-data.com/c-boston)|116250|39780|76470|375|
|60|[Baltimore](https://jobs-in-data.com/c-baltimore)|103000|26544|76456|89|
|61|[Hartford](https://jobs-in-data.com/c-hartford)|101250|25068|76182|153|
|62|[New York](https://jobs-in-data.com/c-new%20york)|115000|39324|75676|2457|
|63|[Santa Ana](https://jobs-in-data.com/c-santa%20ana)|105000|30216|74784|49|
|64|[Richmond](https://jobs-in-data.com/c-richmond)|100418|25692|74726|79|
|65|[Newark](https://jobs-in-data.com/c-newark)|98148|23544|74604|121|
|66|[Tampa](https://jobs-in-data.com/c-tampa)|105515|31104|74411|476|
|67|[Salt Lake City](https://jobs-in-data.com/c-salt%20lake%20city)|100550|27492|73058|78|
|68|[Norfolk](https://jobs-in-data.com/c-norfolk)|104825|32952|71873|76|
|69|[Indianapolis](https://jobs-in-data.com/c-indianapolis)|97500|25776|71724|101|
|70|[Eden Prairie](https://jobs-in-data.com/c-eden%20prairie)|100450|29064|71386|62|
|71|[Chicago](https://jobs-in-data.com/c-chicago)|102500|31356|71144|435|
|72|[Waltham](https://jobs-in-data.com/c-waltham)|104712|33996|70716|40|
|73|[New Castle](https://jobs-in-data.com/c-new%20castle)|94325|23784|70541|46|
|74|[Alexandria](https://jobs-in-data.com/c-alexandria)|107150|36720|70430|105|
|75|[Aurora](https://jobs-in-data.com/c-aurora)|100000|30396|69604|83|
|76|[Deerfield](https://jobs-in-data.com/c-deerfield)|96000|26460|69540|75|
|77|[Reston](https://jobs-in-data.com/c-reston)|101462|32628|68834|273|
|78|[Miami](https://jobs-in-data.com/c-miami)|105000|36420|68580|52|
|79|[Washington](https://jobs-in-data.com/c-washington)|105500|36948|68552|731|
|80|[Suffolk](https://jobs-in-data.com/c-suffolk)|95650|27264|68386|41|
|81|[Palmdale](https://jobs-in-data.com/c-palmdale)|99950|31800|68150|76|
|82|[Milpitas](https://jobs-in-data.com/c-milpitas)|105000|36900|68100|72|
|83|[Roy](https://jobs-in-data.com/c-roy)|93200|25932|67268|110|
|84|[Golden](https://jobs-in-data.com/c-golden)|94450|27192|67258|63|
|85|[Melbourne](https://jobs-in-data.com/c-melbourne)|95650|28404|67246|131|
|86|[Jacksonville](https://jobs-in-data.com/c-jacksonville)|95640|28524|67116|105|
|87|[San Antonio](https://jobs-in-data.com/c-san%20antonio)|93605|26544|67061|142|
|88|[McLean](https://jobs-in-data.com/c-mclean)|124000|57048|66952|792|
|89|[Clearfield](https://jobs-in-data.com/c-clearfield)|93200|26268|66932|53|
|90|[Portage](https://jobs-in-data.com/c-portage)|98850|32215|66635|43|
|91|[Odenton](https://jobs-in-data.com/c-odenton)|109500|43200|66300|77|
|92|[San Diego](https://jobs-in-data.com/c-san%20diego)|107900|41628|66272|503|
|93|[Manhattan Beach](https://jobs-in-data.com/c-manhattan%20beach)|102240|37644|64596|75|
|94|[Englewood](https://jobs-in-data.com/c-englewood)|91153|28140|63013|65|
|95|[Dulles](https://jobs-in-data.com/c-dulles)|107900|45528|62372|47|
|96|[Denver](https://jobs-in-data.com/c-denver)|95000|33252|61748|433|
|97|[Charlottesville](https://jobs-in-data.com/c-charlottesville)|95650|34500|61150|75|
|98|[Redondo Beach](https://jobs-in-data.com/c-redondo%20beach)|106200|45144|61056|121|
|99|[Scottsdale](https://jobs-in-data.com/c-scottsdale)|90500|29496|61004|82|
|100|[Linthicum Heights](https://jobs-in-data.com/c-linthicum%20heights)|104000|44676|59324|94|
|101|[Columbus](https://jobs-in-data.com/c-columbus)|85300|26256|59044|198|
|102|[Irvine](https://jobs-in-data.com/c-irvine)|96900|37896|59004|175|
|103|[Madison](https://jobs-in-data.com/c-madison)|86750|27792|58958|43|
|104|[El Segundo](https://jobs-in-data.com/c-el%20segundo)|101654|42816|58838|121|
|105|[Quantico](https://jobs-in-data.com/c-quantico)|112000|53436|58564|41|
|106|[Chandler](https://jobs-in-data.com/c-chandler)|84700|29184|55516|41|
|107|[Fort Mill](https://jobs-in-data.com/c-fort%20mill)|100050|44736|55314|64|
|108|[Burlington](https://jobs-in-data.com/c-burlington)|83279|28512|54767|55|
|109|[Philadelphia](https://jobs-in-data.com/c-philadelphia)|83932|29232|54700|86|
|110|[Oklahoma City](https://jobs-in-data.com/c-oklahoma%20city)|77725|23556|54169|48|
|111|[Campbell](https://jobs-in-data.com/c-campbell)|93150|40008|53142|98|
|112|[St. Louis](https://jobs-in-data.com/c-st.%20louis)|77562|24744|52818|208|
|113|[Las Vegas](https://jobs-in-data.com/c-las%20vegas)|85000|32400|52600|57|
|114|[Camden](https://jobs-in-data.com/c-camden)|79800|27816|51984|43|
|115|[Omaha](https://jobs-in-data.com/c-omaha)|80000|28080|51920|43|
|116|[Burbank](https://jobs-in-data.com/c-burbank)|89710|38856|50854|63|
|117|[Hoover](https://jobs-in-data.com/c-hoover)|72551|22836|49715|41|
|118|[Woonsocket](https://jobs-in-data.com/c-woonsocket)|74400|25596|48804|49|
|119|[Culver City](https://jobs-in-data.com/c-culver%20city)|82550|34116|48434|45|
|120|[Louisville](https://jobs-in-data.com/c-louisville)|72500|24216|48284|57|
|121|[Saint Paul](https://jobs-in-data.com/c-saint%20paul)|73260|25176|48084|45|
|122|[Fort Belvoir](https://jobs-in-data.com/c-fort%20belvoir)|99000|57048|41952|67|
|123|[Getzville](https://jobs-in-data.com/c-getzville)|64215|37920|26295|135|

&#x200B;",This cost of living can't be accurate! These numbers are basically just rent!,Cool list.¬†Deducting taxes will make it more accurate.¬†¬†,"Of course the best compensation is up north.

I'm in LA but once I finish this MS in DS, I'm moving to the north ASAP.

&#x200B;

My Firm has DS positions but they're hard to come by, it's one of THE SMALLEST teams we have on site. Seeing a Data Scientist is like seeing a unicorn."
1711592901.0,166,Cant land a job in Data Science,"I quit my job in an unrelated field to pursue my dream and failed. I thought I would make it but I didnt.


This is not a rant. Im looking for advice because I feel pretty lost. I honestly dont feel like going back to my field because I dont have it in me. But I cant stay jobless forever. Im having a mental breakdown accepting I may not get into DS so soon because Ive made so many projections about future me as a data guy. Its not easy to let go of them.",What is your education and what was the unrelated field? What have you done in order to make the transition? How old are you?,"Right now everyone and their mother is trying to get into data science. The field is flooded which makes it hard for highly qualified candidates to get a job. 

I don't know your qualifications but I'm guessing they aren't stellar (I'm not trying to insult you here). I have a master in Statistics and a decade of Data Science work including running teams and it took me 6 months and over 100 applications to land a position last time I looked, which was a few years ago. I've heard the market is even tougher now. 

If you just have some online classes you won't land a position as a Data Scientist. You will have to lower your expectations and target a Data or Business analyst position to get your foot in the door.",Shoot for an analyst role and keep trying for a ds job.
1709587252.0,153,Would a FAANG company just use a t-test for A/B Testing?,"Currently doing a job application test for a FAANG company that asks me to conduct an A/B tests and cruelly says:

>There are several equally acceptable stats approaches, so we‚Äôre interested to see your approach.

Which is, like -- *well, I was going to just do a T-Test, but now I'm terrified to try something that simple.*

Any thoughts on what methods a FAANG company would expect on an A/B test like this?","Yeah of course they use t-tests. They also use proportion tests, and sometimes nonparametric tests like permutation tests (though they might not call them that).

Here's stuff that could be relevant to think about for FAANG A/B testing (or A/B testing in general):

1. What kind of metric are you testing? Binary? Numerical? That will have some effect on your decision re: appropriate tests
2. How do you calculate power of the test? Are you measuring rare events? Are the metrics super hard to move? What are good ways to determine the appropriate test size?
3. A FAANG company is typically running thousands of A/B tests simultaneously at any time. How do you keep these tests from affecting each other?
4. Sometimes, just due to random sampling error, a metric will vary slightly from the population mean in the test group. How do you correct for this? Does your correction depend on the type of metric or type of statistical test you used?
5. Typically we A/B test on user experience, and let users be the exchangeable unit. What other things could you test on besides users? How would that change the ways the test is done?
6. Typically metrics are broken down by many demographic groups, countries, ages, etc. What effect does that have on the interpretability of test results (i.e. multiple comparisons)?
7. What is a p-value and how does it relate to significance?

This is all assuming you're going for a DS role. If you're a SWE, I have a whole different bag of questions about logging and A/B frameworks in case you're interested :P","A T or Z approach is pretty universally accepted. Don‚Äôt overthink it and focus on doing it correctly.

One thing though, a lot of these tests will have ‚Äúgotcha‚Äù issues hidden in them. So if they gave you a data set, maybe there‚Äôs a massive outlier skewing the results or something. So take a few minutes just to look for anything weird.","I'm at a FAANG company, and I do t tests. We joke a lot about how people come in and think they need to always apply the most cutting edge complicated tests. Lots don't pass the bar because they over complicate the questions. If you think a t test is appropriate, then by all means use a t test. Just a note though, take a long hard look at your data....there is probably something they put in there to throw you off."
1710966252.0,147,"Learning Python and R at the same time - Pros and Cons, and Do's and Don'ts","EDIT: Thank you for all the amazing insights so far!

Hi all,

The question is for those who have experience with this. I like to have one as a main language and the other as the sidekick. For now I seem to have chosen for Python for several reasons, more courses and tutorials, more articles, larger community. However, R and by extension RStudio/Posit, somehow has a huge attraction to me. Maybe it's their lively Youtube channel, great looking website, ... they just seem to be out there.

I installed both, tried both, chose Python as my main focus. At least once a week RStudio is calling me so I launch it and click around (I like Quarto too btw). But the more I learn Python, the more I find R code to be weird.

In the end I just need to try learning both to find out if it's going to work out, but I like to ask the community first so I can start from a sort of baseline on those with experience in learning them at the same time.

What are the pros and cons, do's and don'ts? Did you basically do everything twice, once in Py and once in R? Or use them for different things, perhaps EDA in R, but then move to Py for ML (or vice versa)? Would that be a good way to learn both, or even make it more complicated?

A bit of background info, I'm learning this in my spare time, neither is used at my current job. Looking at job descriptions on my side of the world, the most asked of the two is Python, some ask for R, some ask for R as a second, and a few stated that either is fine. To me learning a second has merit and potential purpose.

Thanks.","tidyverse and dplyr changed my whole perspective for R, been using it since the past 6 months. I am now scared that I might have lost my proficiency in Python","The best thing about R is it was written by statisticians. The worst thing about R is.....it was written by statisticians. 

It's kind of whack. I love it, but it's different than most languages. For instance, the basic data type is a vector: 

\> haha <- ""abc""  
\> length(haha)  
\[1\] 1  
\> str(haha)  
 chr ""abc""

\> nchar(haha)  
\[1\] 3

So, ""haha"" is a character vector, of length 1, with 3 individual characters. 

I've had a hard time learning Python, honestly. Just....like, why? I like R!

Weirdly enough, spending a bunch of time in PHP has helped me learn Python, a bunch of things just clicked. 

For most folks.....Python is fine, more opportunities, as much as I hate to say that.","Our entire prod modeling pipeline is in R. It is a weird language and some questionable design choices in the language (probably holdovers from when it was S) have indeed bitten us. But I still love it.


If you're going to learn R, make sure you don't just learn tidyverse stuff. You should also get comfortable with base R. I run into situations where one or the other will be way more readable or performant."
1710083880.0,150,Data scientists knowing what data engineers do,"How many data scientists know the ins and outs of what data engineers do? Just curious what your level of understanding or perception is. 

Full disclaimer: I have a training company called data engineer academy ",Every DS I know has a good grip on DE because they spend at least 80% of their time doing data prep and they all complain that they didn't think they'd have to do so much DE work.,[deleted],"Very familiar. Any DS worth their salt knows how the data they‚Äôre working with goes from source to them.

It has major implications on the products that data scientists create."
1705279065.0,148,Data Scientist / ML Engineer Interview Expectation 2024,"How does the interview process for new graduate data scientists compare to that of experienced data scientists (with 2 to 3 years of experience) in well-known, established companies in 2024? Since this field is continuously evolving, I've noticed that some job postings require experience with large language models (LLMs) and hands-on projects.

How much emphasis should I place on various areas such as statistics and probability, data structures and algorithms, machine learning algorithms, deep learning algorithms, concepts related to natural language processing, vision, time series, recommendation systems, and clustering?

Given the challenges of securing interview calls, especially with the need for sponsorship, how should I prepare for these interviews? Any tips and tricks would be greatly appreciated.","Without knowing the company in particular, here's some general interview advice for DS in 2024:

Yes, LLMs are amazing, yes LLMs are hot right now, and YES they do show up in job descriptions. But in 2024 many jobs aren't actually needing in-depth experience with LLMs. They add LLM to the job description to seem sexy, because every exec is asking their team ""what's our AI strategy"" and every competitor claims to infuse AI into their products. Also, adding genAI/LLM keywords into a job description is a great way to get people to apply to the job, especially at more boring/traditional/older companies that want to seem hip and cutting edge.

Now, when it comes to the interviews, I've seen that **IF** they ask about LLMs, they'll ask more project-based questions and start casually like ""oh you fine-tuned an LLM for fun or in your last work project, that's awesome, how was it?""

Then they'll strategically ask follow-up questions to gauge your depth:

* What challenges did you face with fine-tuning your LLM?
* How did you evaluate the effectiveness of the LLM?
* Oh have you heard of RAG? Oh awesome did you try it? Did it help?

Caveat: they won't be asking these casual questions at OpenAI or Anthropic or Google Brain... but 99% of us aren't interviewing at those companies.

For most companies, even in 2024, their DS interviews resort to basics like:

* tell me about regression
* tell me about L1 vs L2 penalty
* tell me a bit about the bias-variance tradeoff
* let's do some Python Data Structures questions or Pandas data cleaning or some SQL window functions depending on the role

And then, they'll ask some more questions based on what you have on your resume:

* Talk to me about the last ML model you deployed...how did you clean the data? why did you choose Neural Networks? how did you handle re-training and model drift?  did your PMs/customers care about explainability of the model, and how did you handle that?
* Oh did you use Spark.. tell me 2 annoying things about Spark?
* Oh I see you listed both PyTorch and Tensorflow.. which do you like better and why? How did you get your team to transition?

Shameless plug: for more thoughts on interviews, wrote 301 pages about this exact topic in the book [Ace the Data Science Interview](https://www.acethedatascienceinterview.com/) and made DataLemur to practice [SQL interview questions](https://datalemur.com/questions) for free",just know everything about everything and you're good to go,Get a good grip on the concepts you have mentioned in the CV and be as authentic as possible.
1704438397.0,145,Is imposter syndrome in data analytics/science common?,"I‚Äôm [M27] currently a Senior Data Analyst in the public sector in the UK. My background was a Physics degree, Physics PhD (involving data analysis), a 2 year stint as a Junior Data Analyst after that, and I recently landed my Senior role.

Despite it going very well for me on paper (and in practice - I have never had any performance concerns raised, and have been praised for my work) I constantly feel like I‚Äôm not good enough. It feels like there‚Äôs always just *too much* to know and remember, whether it be different programming languages or mathematical/statistical approaches. You‚Äôve got programming languages like SQL, R, Python, tools like Excel and Power BI, version control platforms like GitHub, and that‚Äôs before you get into the world of statistics and statistical techniques (descriptive stats, inferential stats, predictive modelling, etc.), and data visualisation. And this is even before you have to get to grips with the datasets you‚Äôre working with and the wider context.

The problem is, it just seems impossible to know and retain all this information, especially when I‚Äôm not using it all daily - yet I put this pressure on myself to be a fountain of knowledge for all things data analysis because you‚Äôre supposed to ‚Äúgain experience and develop‚Äù throughout your career. So why do I feel like I‚Äôm actively getting worse and forgetting things every day? I basically feel like ‚Äúme of yesterday‚Äù was sharper/cleverer than the ‚Äúme of today‚Äù.

Are these normal thoughts? 

Part of me wonders if it‚Äôs due to my background being physics (also forgotten most of that now despite doing 7 years of it), and not directly statistics, or do people in other technical fields with relevant backgrounds have these thoughts too?","It's pretty normal I think. With time, you just get better at accepting what you know and don't know and get the confidence that if you encounter something you don't know, you will learn it in reasonable time if really necessary.","It‚Äôs either imposter syndrome, or superiority complex. Choose your poison. ‚ò†Ô∏è","It's very common, especially because it's difficult to tell if you're having an impact because so much of your work's impact is subjective and it's about helping in decision making. 

There is also an element of feeling that you have little control over outcomes as you're expected to influence without authority.

To any one reading this, I don't want to have to ask for this but could you please upvote this comment? I'm trying to make a post but don't have sufficient karma to do so."
1707549882.0,141,What IDE you use for data analysis?,Jupyter Notebook is one of the most used IDE for data analysis. I am curious to know what are other popular options.,"VSCode (with extensions) is popular, I switched because of copilot lmao.",Rstudio,Pycharm. It‚Äôs significantly more intuitive for python than vscode
1705337907.0,138,"The Hard Truth about Artificial Intelligence in Healthcare: Clinical Effectiveness is Everything, not Flashy Tech","Hi all, I think the following [blog post I wrote](https://open.substack.com/pub/mlinhealthcare/p/the-hard-truth-about-artificial-intelligence?r=7bxky&utm_campaign=post&utm_medium=web) may be helpful to a lot of people in this sub who work in the healthcare domain! Here's a quick blurb about the article.

AI in healthcare faces a critical issue: our obsession with cutting-edge technology often overshadows the actual impact on patients. Successfully bringing AI medical devices to market entails much more than excellent diagnostic performance; it requires rigorous clinical trials and comprehensive cost-effectiveness analyses. HeartFlow's AI-powered cardiac imaging product FFRCT is a perfect example of that. In this blog post, I critically review FFRCT and discuss broad lessons for the future of AI medical devices.

If you're interested in evidence-based medicine, AI/ML, health economics, and envisioning the future of healthcare, this blog post is for you. What do you think the biggest barrier for AI in healthcare is? Let me know in the comments!","Everyone talks about artificial intelligence in healthcare.

The sad reality is we haven‚Äôt even gotten the basics right. Virtually none of the systems talk to each other. Doing simple things like sending referrals and booking appointments requires an extremely counterintuitive multistep process. 

There is so much data to be collected and we‚Äôre  not even collecting it properly.",A lot of what‚Äôs needed is just unglamorous shit like making systems usable and data less of a shit show.,"I also work in a healthcare-centered data science role. I like what you‚Äôre saying about determining whether a technology-enabled device is a good fit for a patient - that‚Äôs a lot of my day-to-day. Beyond the obvious ‚Äúdo they fit the medical profile which would benefit from this device‚Äù, the fact of the matter is that healthcare revolves largely around an aging population that has an inherent distrust of technology. We can give patients non-emergency medical devices, and we have no oversight over whether the patient actually interacts with their device and learns to use it, much less if they feel safe with it. The onus will increasingly be on data scientists and physicians to help their patients learn about the tech-enabled devices and feel comfortable using them. As much as we ask questions about ‚Äúdo we think a patient will benefit from this device?‚Äù, we also need to focus on whether we think they‚Äôll actually be open to the idea of using a medical device recommended or powered by AI/ML technology."
1711558398.0,137,"Is it just me, or have there been a lot of data science job postings lately that require skills in data engineering?","Not only with job postings, but I know a few individuals who work as data scientists at reputable companies, and often they are tasked with the responsibilities of a data engineer. I believe the issue stems from a lack of data literacy among companies and data managers.

In terms of job postings, most of them require extensive experience in SQL, data cleaning, ETL, Pipelines and data quality-related tasks, which I believe fall within the realm of data engineering. I would like to hear your thoughts on this. Have any of you experienced something similar or perhaps dealt with it firsthand?","IMO SQL and data cleaning/wrangling seems like foundational skills across the family of data analyst/data scientist/data engineer titles

I think complex ETL/ELT data pipeline work would typically fall under the data engineer role","I would expect data engineers to be responsible for the core data warehouses, data ingestion pipelines, infra/data platforms, and BI tools.  But dude, SQL and data cleaning are table stakes for any kind of analyst or data scientist.",Data scientists who can‚Äôt do DE tasks are going to be redundant
1711120957.0,136,"Used data science to predict my bracket, figured why not see if my spreads are close to Vegas.....","This year tried to predict my bracket again with the data from Kaggle.  Then I realized, why not try and predict spreads and see how close I come to Vegas.  If PRED\_SPREAD is positive its in favor of TEAM2 so reading this,  FL Atlantic -4.5 (vegas) to my -4, Baylor at -14.5 (vegas) to my 15 etc....   Anyone else use Data Science to help make their bracket?  I wrote a process in Snowpark (basically spark) that creates my bracket when I run top down.

Techniques used

1. Ingestion - Polars to ingest Kaggle files into Snowflake
2. Feature engineering - Ibis, Snowpark, Pandas
3. Model training - Distributed GridSearchCV XGBoost with SnowparkML

https://preview.redd.it/bjmx92u3kwpc1.png?width=1322&format=png&auto=webp&s=1f874eb8b225d09e46004f803db207fd0d111d88","My model hit 14/16 yesterday. I focused on predicting upsets and the 11 seeds paid off. Hopefully today is just as good. My office pool is comprised of other data scientists, so it‚Äôs models vs models, winner take all.",I tried this a couple years ago and ended up dead last in my office pool. Now I just make my picks based on 60/40 weighting of seed and which mascot would win in a fight.,"I build a model, but I don‚Äôt predict game spreads or head-to-head outcomes, because I don‚Äôt really know who will be remaining in the tournament in the later rounds. Instead I predict how far into the tournament each team is likely to go based on their efficiency measures, with training data going back to the mid-2000s. In a head-to-head match up, I just take the winner to be who I predict to go deeper into the tournament. That worked pretty well for me in my pool the last two years, so I‚Äôm doing it again this year."
1707691038.0,138,"How many years do you think until data science, analysis, and engineering(MLE, etc.) roles split into domain specific roles?","Data science already requires a ton of domain knowledge to go beyond a junior level in the current job market(and descriptions!), but when do you think it will no longer be commonplace to see a job for ‚Äúdata scientist‚Äù or ‚Äúdata engineer‚Äù with common requirements +preferred experience and see more jobs like ‚Äúcontent delivery algorithm scientist‚Äù , ‚Äúmedical support optimization analyst‚Äù or a ‚ÄúSalesforce data pipelining engineer‚Äù?  I know that we do already see roles like this, just wondering what y‚Äôall think on how many years it‚Äôll be before data science goes the way of the business analyst and IT specialists.",I think it starting right now,I'm in bioinformatics. We even have our own name!,"Already started. I'm a ""People Data Scientist"" lol. Dumb name but basically denotes that our team focuses specifically on people-centric data (predicting job performance, building candidate selection models, forecasting employee attrition, etc). Similarly, our product-facing DS all have titles like ""Dynamic Pricing DS"" or ""Search and Matching DS"""
1707074934.0,133,Do any of you guys ever need to ‚Äúget permission‚Äù to use Bayesian methods at work or do you just do it,"I‚Äôm seriously thinking, like if I really wanted to, for the problems I work on if I know it can be framed in a Bayesian hierarchical context I am just going to do it and show the stakeholder posterior distributions of kpis etc. if anything they will be thanking me for being able to interpret a credible interval probabilistically, instead of *misinterpreting* a confidence interval by speaking probabilistically.","I could tell my boss I was going to use magnetic inference and he would just nod at me, he wouldn‚Äôt know a prior from a pumpkin

From his perspective my entire team‚Äôs work is voodoo,  we are a black box that data goes into and business value comes out. As long as we do actually generate enough value to justify our salaries we get left mostly alone.",Using Bayesian methods without permission is a Class E Felony.,"If I asked my boss if I could ""use Bayesian methods"" he'd think I was either trying out some new SCRUM thing or microdosing"
1705013549.0,133,Is it normal to spend a day on something that doesn't work?,"I recently started a co-op at a large retail organization and was working on my first real assignment, which involved trying to find a way to improve accuracy of a model (I dont want to get much more detailed, but i know that's not very clear lol). I spent the day trying out a few methods, testing, gathering results, etc, but all of the things i tried, including things suggested by my manager, either performed the same or _marginally_ better, and didnt really accomplish the large scale goal we were trying to achieve. I was truly working and actually ""doing stuff,"" so i wasnt slacking off, but it doesn't feel like I have much to show for it. Is this normal or did I do something wrong/do ""bad"" DS? I know stuff like this is iterative, but I'm wondering if things like this are supposed to be quick and I moved too slowly.",sometimes i spend a week and have more problems than when i started,Come back when it's been more than a month.,"the implicit premise of this question is that most ds initiatives eventually work, which, lol"
1710000163.0,130,shouldn't we be spending more time creating models and business logic instead of deploying code?,"As data scientists, a majority of our work should be building valuable models and business logic that can drive impactful decisions. In my experience I'm either spending too much time deploying my code over a large number of computers or I'm waiting on a DevOps team to do it. I constantly feel the deployment overhead eating away at the work I SHOULD be doing and WANT to be doing. Do any of you feel the same way?

Also, if you leverage any useful products that allow you to easily scale code in production please drop them here. I'm always looking to try new things.

^(P.S. this is the) [^(abstraction)](https://www.burla.dev/) ^(I'm collaborating with my DevOps roommate on \~ people were DMing about it)",You can build all the modes you want but if they aren't deployed and drive value then they are worthless.,"The DS role is not set in stone. It is very different depending on where you work and even what team within the same company. It is not unheard of to be a full stack DS involved in deployment, or even be a DS that doesnt do any models.¬†¬†

Ultimately, there is a difference between the role that you want and what the role is. Any changes to what you should be doing should be based on what is in your company‚Äôs best interest, not yours. If you can do that, make a case for changes, accept it, or change jobs.",[deleted]
1709460506.0,125,An interesting question popped up during an interview,"Was interviewing for a data scientist position, one of the team members asked ""Given your ideal job, which job tasks would not be on that list?"" Interested what you all think ","- Asking for access to data with multiple forms
- Convincing higher management that they need a team for data science and not just one person","I have always maintained that data science at the end of the day is about insights for driving business growth - it‚Äôs akin to the Big 3 consultants just that it‚Äôs based on hard facts and not bullshit (Not me, that‚Äôs John Oliver and go said that). 

So when I‚Äôm driving business growth, I don‚Äôt mind any tasks on my plate as long as I have the agency to fulfil said tasks. If your org. Is too fragmented and too siloed, don‚Äôt look to me to break that unless you‚Äôve given me the agency with the right level of buy-in. I don‚Äôt want to be antagonising people making them feel like I‚Äôm stepping on their toes. 

If your org. is too much into collaborative, cross-functional decision making, don‚Äôt look to me drive super-quick change, because your org. culture demands that I involve all the concerned  players in the decision making process, unless you‚Äôve made me a product owner or something equivalent with the agency to take the final calls.","Any task where I serve as a human ETL process a la this sadly very relevant XKCD comic:

https://xkcd.com/2565/

Sometimes this is unavoidable - someone has a need for a bespoke batch of raw data and your data science team knows where it lives and has the right credentials. But when it happens more than perhaps 2 times for the same dataset and there's no movement to get it automated, I start getting annoyed!"
1704246623.0,128,I love Python/am good at it but I don‚Äôt understand SQL,"Hi, I need to get better at SQL.  I‚Äôve been doing Leetcode for 1.5+ months now and barely any progress. 

I love python and am great at it. I don‚Äôt understand why SQL is difficult for me to get a grasp on. 

Any good Youtube channels/websites/resources to understand/get better at SQL? 

I want to switch jobs later this year and need to get better at SQL asap. Thanks!","Where do you find yourself struggling? Don't want to suggest something like w3schools if you have basic syntax down, for example. [Khan academy](https://www.khanacademy.org/computing/computer-programming/sql) has some more advanced SQL concepts.","I'd suggest setting up PostgreSQL (or better yet, SQLite since it already comes with Python) on your own machine, putting together a simple database (maybe 3 tables or so) of synthetic data, and just starting to mess around with queries to build some intuition behind them.  You can progress to WHEREs, JOINs, WITH AS, summary statistics, etc. as you gain familiarity and comfort with SQL.","The final undestanding behind ALL SQL is Set Theory.
That simple thing helped me to never struggle in any logic required in my queries.
Please familiarize yourself with Set Theory and set operations. Every JOIN and WHERE statements will becone second nature."
1711405187.0,127,Why did you get into data science? ,"I‚Äôm currently a sr. Data analyst, love my job and I‚Äôve come to appreciate the power of analytics in a business setting . When I first went to school I spent time as a data scientist which was equally as enjoyable for different reasons. 

What I‚Äôve seen in the real world is data science has difficulty in generating business value and can be disconnected from business drivers. While I don‚Äôt disagree that work done by data science can be critical for some companies, I‚Äôve seen many companies get more value from analytics and experimentation. 

There has been some discussion that the natural progression in the field is to go from data analyst to data scientist, but why? In companies I‚Äôve worked for DS and DA were paid on the same technical level while usually working more hours( this goes for DE as well), so the move can‚Äôt be for the $. 

For those in data science, why did you chose that route vs analytics. For those that transitioned from DA to DS, did you feel like you made the right choice? ","I looked at the salary guides. I don‚Äôt know what companies value DS and DA the same, but I‚Äôm glad they exist.

Data Analytics is a key piece to data science. It should go: Data minded business executives, upskilled IT team, strong data analytics, then data science.

Too many firms like to try to do all 4 at once and then can‚Äôt produce any value.",Math and money,"The value obtained from a data analyst versus a data scientist entirely depends on the company and industry you work for. Generally speaking:

Data analysts are great for every day cost savings, product testing and improvements, customer optimisation etc. Most marketing and finance companies will get good value from a data analyst on site who is working across product, customer, and financial data. It‚Äôs often an easy hire too because the results are quick and tangible to upper management. If your personal strengths lie in business acumen, data mining, and statistics this can be a lucrative and enjoyable career path. 

Data scientists can be a little more difficult to evaluate from a profitability standpoint because their value and output has to be considered over a much longer timespan, usually. Their work is also often less tangible for hiring managers and C-Suite to comprehend, which can be an issue for data scientists when they join an environment with like this. It can become an uphill battle for the DS to prove their worth compared to their analytics counterparts who are delivering reports and spreadsheets daily. In my experience this is very rarely due to a lack of skill with the DS and rather a lack of understanding with management or a lack of clarity in terms of the project. I‚Äôve seen things turn sour for DS‚Äôs in this situation who eventually move on to greener pastures. Generally if you‚Äôre stronger in computer science than you are in business acumen, I‚Äôd say a DS role is better suited to you than a DA role. But I think data scientists have to be far more selective about where they choose to work due to the reasons above. However DS‚Äôs can often command a higher wage. 

So there‚Äôs pros and cons to both routes and although there is overlap in skill set, I‚Äôve found there is a distinction in personality types and strengths when it comes to DA‚Äôs versus DS‚Äôs.  

Worth noting that (in the UK anyway) it‚Äôs much easier to claim R&D tax rebates on a data scientist than it is a data analyst. This can be a very attractive feature when you consider that a large chunk of the DS‚Äôs salary can be claimed back as a cash refund from HMRC, as well as anything the DS touches (software, hardware, cloud server costs, other members of the product team etc)."
1707312045.0,128,One Trillion Row Challenge (1 TRC),"I really liked the simplicity of the [One Billion Row Challenge (1BRC)](https://github.com/gunnarmorling/1brc) that took off last month.  It was fun to see lots of people apply different tools to the same simple-yet-clear problem ‚ÄúHow do you parse, process, and aggregate a large CSV file as quickly as possible?‚Äù

For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset üôÇ.  Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.

We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see [this blogpost](https://medium.com/coiled-hq/one-trillion-row-challenge-5bfd4c3b8aef) and [this repository](https://github.com/coiled/1trc/)",[deleted],"Has anyone tried this with polars? 

Obviously I know that it won‚Äôt be anywhere near as performant as the top entries.

just interested to see how long it takes for polars compared to dask, pandas 2.0 and other typical python libraries","This looks like a cool challenge to try Polars, Spark and Mojo."
1704395903.0,124,Update half a year into my first data science job...,"As with the title, I started my first data science job about seven months ago and I wanted to share my experience for anyone considering getting into the field.

It has been amazing so far! For the first time in my life, I can honestly say that I love my job.

I'm a fully remote Data Scientist working for a big research university, and it's the best working experience I've ever had. My boss is super chill, always looking out for me, and makes sure I don't get overworked. They're also always encouraging me to use down time to keep learning, and to work on back burner projects that interest me and utilize all of my skills so I can keep sharp.

As for the work, I love getting to work with lots of data and to build/train models to provide actionable data insights and answer important questions - sometimes I feel like a detective. Further, I love my ""clients"" (university leadership), and they're always appreciative of my work (honestly, they seem to look at what I do like it's magic haha). I also feel like my work is actually useful and makes a difference for the university since it gets used in institution-wide decision making, accreditation, gets published on the website, etc.

I love that they don't care ""when"" I work as long as the work gets done (i.e., I can run errands mid morning if I feel like it, etc.) - there's a ton of freedom/autonomy. And I love statistics/numbers, and I feel like I get to be somewhat creative. I will say, however, I'm in lots of Zoom meetings that should maybe be emails lol, but it's sometimes nice just to get a chance to talk to people since we're remote (I kind of feel like that's what the meetings are for).

Finally, the pay and benefits are amazing. A lot of the time, I'm amazed by it lol (particularly since I came from a lower-paying, higher-workload and higher-stress field - clinical research - it's night and day).

So, how did I get into it? I'm doing my master's in DS and have a bachelor's degree in econometrics, but I also think the bulk of how I got the job was from doing projects and being able to show that I knew what I was doing via a portfolio/giving concrete examples of competency. (I mostly used Python/R, SQL, and Tableau. Also Jupyter notebooks, and Excel and PowerPoint are probably good to know, etc.)","This is amazing to hear. I‚Äôm interested a lot. If you mind disclosing, how much money do you make, at least within a 10-15k range? For your first ever job. (And did u go to a good university for undergrad and grad)

Also, I‚Äôm thinking of going into data science. My university offers a cool degree called Finance + Data Science and I‚Äôm trying to apply into it because I‚Äôm interested in both subjects, and I have a strong quantitative background. I really want to master programming languages for DS like python, R, etc and other resources like Tableau. I‚Äôm also pretty good at statistics and I‚Äôd say I definitely like it too. Is it okay for math to only go up to calc 2 and linear algebra? Ik data science is a quantitative field but doesn‚Äôt always need to be an expert on math? If I need calc 3, I‚Äôll take it at CC. 

Also for remote jobs, that‚Äôs cool with the flexibility. Do you think there are some jobs in the field that may be a hybrid, where you can do some work at home and also go in person? Sounds like a dream job. I wouldn‚Äôt wanna be all at home because I might go crazy inside all day, but the flexibility would be nice and idk if I‚Äôd wanna commute every single day 5 days a week.

Btw, what is your workweek in hours? Id want a job that works 40 hours to give good life balance . In terms of getting started with your career, I assume it‚Äôs generally normal to work 40 hours and not try to overwork like 50-60 to ‚Äúget ahead‚Äù in your career.

Also, since you mentioned you had a degree in econometrics, is your job actually related to it or are you just using your masters in DS (and obviously the knowledge with statistics learned in econometrics)? Like are you doing data science within the field of economics? 
And can you give a rundown of the kinds of projects and such you may need to do as a data scientist? Do you often work alone or with other people?

And did you know how to code during undergrad or did you learn all of it during your masters? Lastly, I think going to zooms and getting emails is alright but are u in those meetings to present your findings or what are they for?

I know that people tend to jump from career to career to gain more experience, but it sounds like you love your job. Would you be willing to stay for many years and if not, why not?

Lastly, can you expand on what kinds of projects you would do for data science? You mentioned it could have been a relevant factor in your getting of the job despite your degrees.","Do you have any recommendations on what types of projects folks who aspire a similar job should work on, where they would find them, etc.?",What was your experience prior to this? Also can I DM you? I‚Äôm an econometrics focused applying with a large portfolio but lacking outside of academic research
1706920451.0,123,Has anybody played with Astrological sign as a feature?,"Now that I've got your attention.

Astrology, while largely a pseudoscience that appeals to non data minded folk, is largely about identifying trends and presenting them in a human readable (albeit unscientific) format.

While it's unlikely that the placement of the stars and planets themselves have a significant impact on the trajectory of an individual, the placing of their birthdate within the year (which I suppose is in relation to the sun) probably has an impact on how they develop and find their way within society.

For example, someone born in August tends to start school almost a year earlier in their development than an individual born in September. From there the August child will always be amongst the youngest in their class whereas the September child is grouped with the oldest. This has cascading effects throughout childhood development that will leave drastically different imprints upon both children and create different adult life paths.

So now that I've gone through that long winded explanation to justify my buzzy title, does anybody have experience playing with birthdate (specifically time of year, as opposed to age) as a feature?","This is something that comes up in sports because of the way youth leagues are sometimes run. Typically kids will be grouped by age with an annual cutoff date. Those with birthdays soon after the cutoff are at an advantage, because they will be among the oldest in their age group, and therefore the largest. This feeds into a cycle of the most successful kids getting the most attention and coaching. This ultimately leads to a selection bias within pro sports based on birthdate.

Here's a reddit thread that shows the distribution of NHL Hockey players based on month of birth: https://www.reddit.com/r/hockey/comments/il72c1/birth\_month\_of\_nhl\_players\_born\_since\_1980/","20 years ago one of my first bosses always made us put star sign into the model building mix. 

It would occasionally be significant, and the lesson he was trying to teach was that domain knowledge and model explainability were important (at least in our domain) because occasionally random noise was significant enough to be selected by the model algorithms. 

Great guy, learned a lot from him.","> For example, someone bon in August tends to start school almost a year earlier in their development than an individual born in September.

This impact is profound.  Others have talked about sports (birthdates for Little League eligibility on MLB players birthdays, also Canadian junior league hockey.  

The decision to delay kindergarten by a year is a strategy used by a lot of students.  The theory is that the additional maturity in the child leads to long-term better performance and confidence.  

There is a lot of economic and social science research in these areas.  If you are doing anything in that area, or related fields (education, psychology...) it's worth considering as a variable."
1708122423.0,119,Why did you choose data science vs. some other software engineering/development discipline?,"There was a post on this about an hour ago. I thought this was a great question for a Friday, but sadly OP deleted the original post. 

So why did you choose data science over software engineering?","Having tried both, I found I liked doing statistics, visualizing data, writing up insights, et cetera, a lot more than I liked the process of coding systems. Programming felt like something I enjoyed when I was very invested in the end product, which is great as a hobby but not great for a career. For data science, I've worked on things that I didn't give a shit about but I still enjoyed the work - which makes it a good thing to build a career on :)","Because I‚Äôm good at math but a poor software engineer, and this seemed like a reasonably easy path to high salaries. Pretty simple, lol","It's easier to teach a PhD scientist how to code ""well enough"" for Data Science than it is to learn how to be a software engineer. And for people who state with absolute fact that you need to be a software engineer to be a data scientist, I encourage you to look outside of big tech companies and high tech startups.

Also being a Sr. Engineer in production is shit. Everything is on fire all of the time constantly, the pay is pretty mediocre for how much stress you're under, the operators are there to put in their 8 hours and go home, being on call sucks, and the commute was terrible for me. I didn't want to do basically the same thing but on a computer instead of with coating chambers."
1707191539.0,121,Analyzing datasets with trillions of records?,"Read a job posting with a biotech firm that's looking for candidates with experience manipulating data with **trillions** of records.

I can't fathom working with datasets that big. Depending on the number of variables, would think it'd be more convenient to draw a random sample?",You don‚Äôt analyze it all at once. Just working with data that big is hard and involves tons of engineering. Take a look at Databricks,"This is coming from a data modelling/DE point of view, but you could pretty easily have a trillion+ rows in partitioned tables with large number of groups and observations (e.g., 20-30 years of daily data). I doubt anyone would try to `SELECT *` it into a dataframe, though.

A real-world example: We create a performance table (my domain is finance) that, if allowed to, would grow to tens of trillions of rows. Simply a matter of creating enough scenarios that are allowed to map many-to-many when it comes to measurement periods (i.e., start on Jan 1st., stop at Jan 2nd, and stop at Jan 3rd. etc., with different calculations for each).

You will almost always pick a couple partitions to pull data from. So, while the ""core table"" is huge, the workspace remains easy to manage.","Data this large can crop up in lots of places: logs or telemetry data from big distributed systems, sensor data from the hard sciences, bioinformatics stuff, etc. 

You‚Äôre right that it‚Äôs often more effective to draw random sample than deal with the whole enchilada at once. But oftentimes depending on the problem space it‚Äôs nontrivial figuring out when and how to draw those samples so as to leverage the data properly/effectively; hence data scientists. 

The general pattern of solving business problems with data this large is mostly the same, it just is more complicated and relies on more high-powered tooling (e.g. data sitting compressed columnar formats in cloud storage, big compute clusters for analyzing that data, aggregating it and depositing the downsampled forms it somewhere else, etc). 

Source: I dealt with datasets of this size for several years as an applied researcher at AWS."
1706527632.0,125,Which path is better: Data Science or Software Engineering?,"Okay, so I'm in need of some career advice, because honestly I'm at a point where I don't really know how to proceed in the future.

For some background info:
- I've completed my Bachelor's degree in Computer Science (3 years degree), took a year of pause (during that time I just focused on work), and at the moment I'm in my first year (out of 2) of my Masters's degree in Data Science.

- As for work experience, I started working during my second year of Bachelor's as a Software Engineer. I've completed two internships, and I've been a Junior Software Engineer for more than a year and a half (currently I'm still working, trying to manage both my job and the university lectures and assignments in parallel). The languages/frameworks that I've worked with the most are Python - Django/FatsAPI, Ruby - Ruby on Rails, TypeScript - React (even if my perspective for this field is that I shouldn't be fixed on just some languages/standards, as the industry is evolving at such a rate, that from a month to the other we might find a new tool that's 100 times more efficient).

- Up until recently I didn't ever interact that much with the concepts of Machine Learning, but since starting the Data Science degree I've realised that I like this field a lot and I take every Uni project as a challenge that I love solving.

Now let's get to the actual issue at hand: in the next few months I would like to look for another workplace, since I've realized that the environment that I'm currently in is affecting me negatively and unfortunately I don't consider that this is a place where I can grow (both as an employee/software engineer as well as a person in general). If you'd like more info about the reasons, I can edit this post and add them, but I don't want to make this too long.

At this point, I don't really know what path to take: continue with the Software Engineering career or switch to Data Science / ML / NLP?

I'm asking this from 2 distinct perspectives:
1. which one of the two is better for my future? I've been reading different posts lately about some data scientists / ml engineers saying that this field is really volatile and a lot of companies switch strategies from one day to another and many find themselves unemployed out of the blue, besides the fact that some people say that there are already too many data scientists / ml engineers for the few jobs required. But thinking about all of the advances in AI that have been happening lately, I was under the impression that this field is going to thrive in the future.
2. I have always believed that salaries in both fields are good, but as a data scientist / ml engineer you can reach a higher max salary compared to what the max salary of a software engineer can be (even if I've also read posts that stated the contrary).

If I want to be completely honest, I think that I already have the answer, since I like a lot what I'm studying and everything related to this field is just sooo captivating. 

But my problem right now is that I've ve been able to get to a point where I'm competent enough as a software engineer to make enough money to support myself, so finding another job in the same field would be easier. Since my parents are not helping me financially anymore, I don't think I would be able to get by with the paycheck of an intern (around 800 Euros/month where I live), if I were to switch fields.
And I don't think anyone would take me in this field for anything more than an internship, because I'm aware of the fact that I do not have enough experience in Machine Learning and Statistics for any other role.

As for some additional information that might be helpful, I'm in my early twenties and I live in Eastern Europe, where prices are not as high as in other places, but it's still not easy to get by by yourself.
I've been also looking around for various opportunities in other countries in this field, in order to be able to work remotely or maybe do an internship there, but I haven't been able to find much.

PS: I know that a lot of people might say that I'm young and that this is the best time to take risks, but going to an intern's salary would just expose me to the risk of being homeless, and that's not such a good option :D

PS 2.0: I have also mentioned the NLP sector because it sparked my interest and I‚Äôve been taking some extra classes in order to gain some more knowledge about it.

TLDR: which is the best career option between Software Engineering and Data Science/ML/NLP?","Become a Software Engineer, it is definitely more stable and you'll have more breath room with what you want to work on or who you want to work with.","just my 2 cents from my boss re: data science. choose one of two: 

1. heavy prob/stats with deep math knowledge. 
2. basically dev with DS knowledge. be able to fully deploy ML models from start to finish. 

anyone in between (fit-transform/notebook ds) are getting shaken out of the market.",[deleted]
1709168601.0,121,Analysts with Weird Degrees: What is it?,"I have a degree in the Arts field. Have been successful in this field and peaked at where I am (mid 30s). Looking to make use of self-taught programming, Tableau, analytics, etc. to get another job/career start. 

If you were in a similar spot with an unrelated degree and unrelated previous career, what was it? Did you manage to find some work in data? 

Would love to hear your story for some inspiration.","I‚Äôm a Director now, but English Literature.

I worked in the business side of an insurance company doing underwriting and claims and then went back to grad school. Wrote what must have been one heck of an admissions essay. All I remember is that my Mom said it was overdone and I should reconsider. It‚Äôs literally the only criticism my Mom has made of my writing (no hard feelings, just stands out). 

When I got to grad school I realized that I had taken 2 math courses in the past 7 years and failed one of them (pre-calculus). So I took a totally different approach. I rewrote every variable, equation and notation into actual words, like a sentence. It totally changed how I saw math and dovetailed nicely with learning to program. 

Did really well in the program, finished very quickly and then did the consulting firm thing. Went back into insurance on the data side and it‚Äôs worked out well so far. 

For scale, this journey from English Major to Director of Data Science took 15 years.","Some of the best analysts I've seen in my career came from  non technical backgrounds.  Coding isn't even half the equation.

Understand the business you're supporting, communicate well, listen critically, ensure the right questions are being asked, and above all have an insatiable curiosity.

Edit:  I have a bachelor's degree in Marketing and I'm now a Director of Analytics.","Masters in Supply chain management and bachelors of business. I had 1 semester of R in undergrad and I liked to do stuff in Excel. I taught myself Tableau and used that and excel skills to talk my way into an analyst job that required Power BI. Spent a bunch of time on YouTube and taught myself Power BI. Quickly hit a limit with Excel and Power BI and broke out my notes from R. Now I do lower level Data science and ML all the time to solve business problems and spend most of my day coding in R. In those 3 years I have also increased salary from 65k to 90k because of those DS skills I've turned into business insights. Stay humble and always be learning but when you deliver something cool, be sure to sell it"
1704212735.0,120,Meeting hate - is this common?,"I like everything about being a Data Scientist except the meetings. Research is my favorite part, both the heads down modeling and the paper reading. Coding is alright. Even bug fixing is tolerable, kind of scary at first, each time, but then really rewarding when you fix the bug. Meeting prep, like making slides and organizing my thoughts, is fun. But the meetings themselves‚Ä¶ just suck. I have a color coded spreadsheet each week showing the times of my meetings, colored by how much they‚Äôre going to suck.

Last minute ad hoc meetings that are unnecessary are awful. Meetings where some other DS or Eng or even business query monkey shares their screen and I watch them work are the absolute worst. Working from home saves me because I can decompress after each meeting. But even when I worked in the office, I loved the work and hated the meetings.

Just wondering if others feel the same.",[deleted],"> I have a color coded spreadsheet each week showing the times of my meetings, colored by how much they‚Äôre going to suck.

Are you sure you're not over-fixating on this? Because generating a spreadsheet just to quantify how much you dislike a key component of your job, communication, sure seems like over-fixating to me. Almost every job has some tedious component, and the good thing about meetings is that there are concrete steps that can be taken to improve everyone's experience (setting agendas, goals, follow-ups).",‚ÄúBusiness query monkey‚Äù you sound like a lovely person to work with.
1708473395.0,113,Thinking like a Data Scientist in my job search. Making this tool public.,"I got tired of reading job descriptions and searching for the keywords ""python"", ""data"" and ""pytorch"". So I made this notebook which can take just about any job board and a few CSS selectors and spits out a ranking far better than what the big aggregators can do. Maybe someone else will find it useful or want to collaborate? I'm deciding to take this minimal example public. Maybe it has commercial viability? Maybe someone here knows?

&#x200B;

[Colab notebook](https://colab.research.google.com/gist/gbwiersum/b112a6534d71d8ae3c360f5a160cc1dc/job-board-scrapenscore.ipynb)

&#x200B;

It's also a demonstration of comparing arbitrarily long documents with true AI. I thought that was cool.

If you reaaaaly like it, maybe hire me? ",That's really cool! Your last line hits too close to home.,Cool app. Would make a nice platform if you could aggregate everything in a web app rather than notebook,That's really smart. Best of luck in your job search!
1704363731.0,117,How do you detect when a data scientist is chasing a wild goose? And how do you prevent them from consuming company resouces unnecessarily?,"Some teams in my organization have empowered data scientists to explore and develop AI/ML use cases, which is a positive initiative, in the sense that data scientistsare now encouraged to engage more with cross functional data. However, we have noticed that this freedom has led to an experimentation spree, resulting in unnecessary expenses and resource allocation. The new data scientists, who joined our org after getting impacted by FAANG layoffs, are insisting on expensive software and cloud technologies that are straining our annual budget. 

This has caused some concern among the more experienced cross-functional data science teams, including mine, who believe that the leadership's generosity towards the new data scientists is misplaced. They strongly opine, although not openly, that the leadership should not be enamored by flashy yet generic AI-ML slide decks and ""data sciency"" quotes being thrown at them by these new age data scientists.  They feel that these inexperienced data scientists are pursuing impractical ideas that do not contribute to the business effectively. 

Additionally, the new data scientists seem uninterested to take-up any other analytical or engineering work apart from coding in  their Jupyter NBs. While it is important for data scientists to experiment, there needs to be a balance and clarity on when to focus and when to halt. Due to lack of data literacy among the leadership, we feel that there is a lack guidelines to prevent inexperienced data scientists from pursuing use cases that do not provide value to the business.

Has anyone ever been in similar situations? Any suggestions on how we can prevent these?","IMO, a DS is chasing a wild goose if they are solving a problem no one is begging them to solve. Technically, there is no problem fit and there is no willing user. 

The notion that ""let's build the technology, then we find a use case for it"" doesn't work 99% of the time. 

The right thing to do IMO is to search for a problem that solves someone's headache. If your DS spends most of their time searching and formalizing such problems, identifying its stakeholders, collecting relevant data for it, then they are doing invaluable work. Even if they are not the ones who solve it.","\> Some teams in my organization have empowered data scientists to explore and develop AI/ML use cases, which is a positive initiative.

&#x200B;

I don't think this is positive. My understanding of the Data Science work has evolved a lot over the years. Having a team that only does Data Science leads to the approach of - we have a tool (Data Science), how do we use it? That is starting from the solution, as opposed to starting with a business problem. Therefore I am a big proponent of cross-functional teams that can build data products E2E and don't always need Data Science (but it should be an option) in which case the Data Scientist can do research that doesn't block the critical path to a first product. This works as especially the first iterations of tools can often get away without too much complex Data Science and it gives Data Science time to research ahead.",Maybe the leaders are betting on this approach and they feel satisfied with the risk /reward tradeoff.
1707727642.0,112,Sesamestreet teaching kids the importance of metadata,,"It's from this old [Kermit the frog newsreporter](https://www.youtube.com/watch?v=E52o2w5eUvw) gag üòÜ ""here are some of the latest scores""",Wow. This is my life in a nutshell.,üòÇ
1705006473.0,110,When all else fails in debugging code‚Ä¶ go back to basics,I presented my teams‚Äô code to this guy (my wife‚Äôs 2023 Christmas present to me) and solved my teams‚Äô problem that had us dead in the water since before the holiday break. This was Lord Raiduck and I‚Äôs first code review workshop session together and I will probably have more in the near future.,Don‚Äôt you mean ‚Äòquack‚Äô to basics?,Where do you get cool rubber duckies like this?,"I retired mine to the bathroom.


Now it watches me in utter horror as I float around the tub naked."
1711464600.0,110,What are the software engineering best practices that we should know?,As the title suggests I am looking how to improve my overall code quality by taking from our not so close cousins SWE. Any learning resources would also be greatly appreciated.,Follow arjancodes and mcoding on youtube to learn more about design patterns and anti patterns. Follow PEP 8. Be more conscious about using appropriate data structures. Follow best practices concepts like KISS and unit testing. Reading code from software devs helps a lot to see how they think and structure things. There's no silver bullet. It depends on where you think your shortcomings are,"The three things i would recommend:

1. Write functions and get good at knowing how to improve function abstraction (this takes a lifetime)

2. Use type annotation with Pydantic (or method dispatch if you're an R user)

3. Get in the habit of testing your functions and learn what you need to test and what doesn't need tests (also takes a lifetime)","Off the top of my head:

Abstractions, OOP, proper unit testing, PEP-8, DRY/KISS"
1708464432.0,109,Got data anyst job in my country's top food retailer thanks to Coursera,"My bachelor degree is a compete waste and piece of shit (public administration) , which I realised after a second year. Fortunately, due to the war in my country, coursera gave access to its courses for free. I started by talking courses on statistics in social science from uni of Amsterdam (brilliant specialization), and gradually realised I kinda like data, so took courses in R, Excel and Sql.

And after like a month of job hunt, I got here and tomorrow is my first day at my first job!

  
I am so grateful to Coursera! ",Which courses did you find the most helpful? I am tutoring some people working through the Google Data Analytics coursera and I‚Äôd love to hear about some courses they could work on after finishing that.,Wow congrats. Can you talk a bit more about your process. What did you talk about in the interview and how did you structure your CV?,Switched from EE to DS thanks to Udemy. God bless Jose Portilla
1705643210.0,111,What is the most versatile regression method?,"TLDR: I worked as a data scientist a couple of years back, for most things throwing XGBoost at it was a simple and good enough solution. Is that still the case, or have there emerged new methods that are similarly ""universal"" (with a massive asterisk)?

To give background to the question, let's start with me. I am a software/ML engineer in Python, R, and Rust and have some data science experience from a couple of years back. Furthermore, I did my undergrad in Econometrics and a graduate degree in Statistics, so I am very familiar with most concepts. I am currently interviewing to switch jobs and the math round and coding round went really well, now I am invited over for a final ""data challenge"" in which I will have roughly 1h and a synthetic dataset with the goal of achieving some sort of prediction.

&#x200B;

My problem is: I am not fluent in data analysis anymore and have not really kept up with recent advancements. Back when was doing DS work, for most use cases using XGBoost was totally fine and received good enough results. This would have definitely been my go-to choice in 2019 to solve the challenge at hand. My question is: In general, is this still a good strategy, or should I have another go-to model?  


Disclaimer: Yes, I am absolutely, 100% aware that different models and machine learning techniques serve different use cases. I have experience as an MLE, but I am not going to build a custom Net for this task given the small scope. I am just looking for something that should handle most reasonable use cases well enough.

&#x200B;

I appreciate any and all insights as well as general tips. The reason why I believe this question is appropriate, is because I want to start a general discussion about which basic model is best for rather standard predictive tasks (regression and classification).","General Additive Model.  Like OLS, but with non-linear functions.","As my former econometrics professor used to say, it's really hard to beat a good OLS regression.","Depends on the criteria.

In terms of predictive power it is still the case that XGBoost (or LightGBM) can be thrown at most predictive problems and outperform other methods.
It has even become more versatile in its usage in that it is now also often applied to time series.

If you are concerned with interpretability as well as predictive power then OLS , GAM etc would be more versatile.
Using explainable AI such as SHAP, LIME etc is messy, so XGBoost falls short here imo."
1705938138.0,109,People who hire!! What are some of THE MUST Projects to have on a CV?,"Building really good projects requires a significant amount of time and effort. I would like to know from the seniors who are involved in the hiring process. What are some (around 4-5) projects that would impress most senior professionals involved in hiring?   


While the work of data scientists varies across companies, the foundational principles remain the same. For example, Recently, I learned from a highly experienced data scientist/engineer that many companies utilize XGBoost for classification and regression tasks. I would like to understand which projects would demonstrate an intermediate level of data science skills in the eyes of potential employers. In addition, you can even share a project which you think is really fascinating.","The ideal portfolio for me isn‚Äôt about the projects or methodologies themselves. It‚Äôs about how the methodologies relate to the problem at hand. The best projects are the ones where you started with a standalone problem/question and used data science to solve it, rather than projects where the starting point was ‚Äúwhat‚Äôs something I can do to demonstrate neural networks?‚Äù The former demonstrates your ability to do the job, while the latter only demonstrates your ability to write some code.


The latter category also has a tendency to devolve into an analysis where you‚Äôre showing plots and stats for the sake of showing plots and stats, and not actually using them for anything. As a pseudo-stakeholder for the portfolio project, I really don‚Äôt care how much money customers spend on average in a transaction. I probably could have guessed that with napkin math, and it doesn‚Äôt tell me whether I need to change anything or how to change it. I want to know something actionable like ‚Äúhow much revenue should I plan to lose to theft this month, and what are people going to steal?‚Äù These actionable insights are very difficult to produce when you start by finding a topic that suits a methodology rather than the other way around.","Even if you don't have professional experience, anything approaching a ""real life"" project beats one on a toy dataset or something from Kaggle. Pick something you're interested in, can find some data for, and try to think of some kind of ML related use case. Then do the project as if you were actually working on it.

Like someone said above, think about how you would evaluate it and make decisions about it if it was a real-world professional project, do a decent job and it'll be world's away from mist other candidates projects. Even if the results don't blow anyone's socks off.",[deleted]
1704836124.0,103,No senior data scientist at company,"I got hired as a data scientist, and there is no senior person I could talk to.

I feel lost, and this is also my first data science job, too. It‚Äôs also the company‚Äôs first data scientist.

I‚Äôm scared of messing up, not knowing enough, or creating a bad model.",I would focus on trying to understand what the company expects and how you can shape those expectations.,"Did you pivot into Data Science or do you have some formal education? Have you had any Data Analyst or similar roles before?

I‚Äôve definitely been there before. It can be a scary place to be, but the freedom you have is also valuable. Some first time Data Scientists get hired onto strict teams in a Junior role and don‚Äôt have any freedom to be creative and it stifles them. You‚Äôve got the opposite situation that many new DSs might kill for! Even though it‚Äôs daunting, I would try to stand tall and rely on as much training as you have.","The classic OG Data Science role is a research based role.  You're not paid to go in knowing everything, you're paid to learn.  Take your time and take notes.  Learn everything you can.  It's okay.  You got this."
1706060489.0,107,I made a directory of all the best data science tools.,"Hey guys, made a directory of the best data science tools to use in categories like ETL, databases/warehouses and data manipulation and more. I‚Äôm hoping this can be collaborative so feel free so submit projects you use / your own projects. Happy to hear any feedback.

[datasciencestack.co](http://datasciencestack.co/)

https://preview.redd.it/mgbdac9qlaec1.png?width=1615&format=png&auto=webp&s=0182bfb3ab063c71b8aefda7ee3ea9236a593786",I get the sense that this list is more about maximising the diversity of tools than the actual practicality and value of it from an organisational perspective. The comments confirms that.,Gotta have tidymodels and timetk here,"u/alexellman

Mage - ETL  
Polars - Data Manipulation  
Folium - maps"
1710714882.0,102,I‚Äôm really getting frustrated with my career trajectory.,"I‚Äôm hoping to get some career advice. I was a special operator in the military on active duty, the kind you go through selection for, and did intelligence work when I was much younger.  I then transitioned to officer where I was managing a couple of large intelligence cells at up to division level. When I got out and was pursuing a masters I managed two very large restaurants as a general manager. After graduating I became a data scientist where I applied my work toward national security problems as a contractor. As an individual contributor I often worked with some high level military leaders. 

I left to go work at a tech company as an individual contributor because i wanted the credentials of having worked in tech and the money was good. I expected to rapidly grow here into leadership but I feel my role is stagnant and I‚Äôm not growing as a leader nor do I feel the opportunities are going to present themselves. I want to be in a role where I can help by making leadership decisions for an organization and managing teams but I feel stuck. I fully expected data science to help me in my leadership ambitions because you understand the technical aspects far better but it hasn‚Äôt been in the cards. The money here is good but I don‚Äôt enjoy not being a decision maker. 

Not that I don‚Äôt think PMs are valuable but it frustrates me when I end up with someone with very little practical experience sitting over me as a PM. 

I dunno maybe I‚Äôm just being jealous because I took this path over a PM path.

Anyway, I don‚Äôt know. Should I unwind and back up and try a different trajectory? ","Go do something that isn‚Äôt work. Sounds like you‚Äôre already in a good spot.  Or hop companies to a leadership position, apply aggressively for relevant managerial positions leveraging your wide technical background.",[deleted],"If you still hold a clearance (or haven‚Äôt done anything since separating that would preclude you from getting re-cleared) you could join one of several companies as a forward deployed engineer working with the DoD. The path from there towards owning a customer implementation, and from there to owning an account and ultimately a vertical is usually pretty clear (though gated on your ability to deliver results and likely a healthy amount of politics). 

Palantir, Vannevar, and BCG X, all have open roles currently. Scale AI and Anduril might also but I‚Äôm not sure how directly applicable a DS background would be to them though being a veteran will probably carry a lot of weight.

Regardless I think it‚Äôs worth stating that military experience doesn‚Äôt translate 1:1 to commercial. Expecting to be given decision making authority and a big team because you had a command is going to leave you disappointed."
1711578000.0,98,Found a company asking for high school certificates for a Data Scientist role.,"&#x200B;

https://preview.redd.it/2qy68tawbyqc1.png?width=1322&format=png&auto=webp&s=2e9d875eb6fb7d11e14e9e1d7fa91180c6f67eb8",Going to record a video of my high school grade list brb,Personal motivation video is crazy too,"I‚Äôm guessing they ask this for all candidates not just DS roles. 

My high school doesn‚Äôt even exist anymore, where would I even get a transcript."
1707178897.0,101,Avoiding Jupyter Notebooks entirely and doing everything in .py files?,"I don't mean just for production, I mean for the entire algo development process, relying on .py files and PyCharm for everything. Does anyone do this? PyCharm has really powerful debugging features to let you examine variable contents. The biggest disadvantage for me might be having to execute segments of code at a time by setting a bunch of breakpoints. I use .value\_counts() constantly as well, and it seems inconvenient to have to rerun my entire code to examine output changes from minor input changes.

Or maybe I just have to adjust my workflow. Thoughts on using .py files + PyCharm (or IDE of choice) for everything as a DS?",Hope I don't get crucified for this but I typically do all my work in notebooks and then finalize a script when I know everything works,""" The biggest disadvantage for me might be having to execute segments of code at a time by setting a bunch of breakpoints. I use .value_counts() constantly as well, and it seems inconvenient to have to rerun my entire code to examine output changes from minor input changes. ""


Congrats, you learned why people use notebooks.


You can write .py files and call them from your notebook, you know?


Also, you can move to vs code. Have a jupyter note book open in one tab, .py files in others, and %run the .py files as needed.


It's rarely all or nothing with modern IDE's","üôã‚Äç‚ôÇÔ∏è I half do this in the sense that I still use ""#%%"" in .py files in VScode which is basically.using a notebook BUT I like that there is no output saved by default and you can still run it as a script without extra worküëç I am sure pycharm has something similar."
1705410706.0,100,How do you feel confident at work when others are smarter?,"This isn't about the lack of desire to keep learning. I feel like at least for the moment, I'm way under-qualified and under-skilled than my coworkers. 
That doesn't mean I'm always wrong or they're always right, but I feel that way and then don't take any stand at work. It also doesn't help that I've had the distinct misfortune of working in a toxic workplace and these coworkers are also difficult and stubborn and impatient.

I'm sure I'm not the only one considering how many people from non technical backgrounds have made it into data science.

Are there any confidence hacks that you did? I think I'm looking for more mindset hacks than knowledge ones, just from like minded individuals. :)",If people are smarter than me it means I‚Äôm in the right place,"when you encounter something new that you dont know, whether it‚Äôs an equation or algorithm that is spoken of, research it thoroughly until you understand it and can speak of it. 

things are usually not that difficult to understand, it just takes time. they probably are not smarter than you, just exposed to more","Think like an offensive lineman, not like a baseball hitter. You don't have to be the star to be a solid individual contributor."
1711404065.0,102,Got rejected from Analytics engineering role because of having marketing experience (which I don‚Äôt have),"I‚Äôm an Analytics Engineer / Sr. Data Analyst with one of the big tech companies from Australia, although working remotely from Canada. I was applying for a Staff Analytics Engineer role, had the recruiter interview, had the interview with the hiring manager. Everything went well, he said that I‚Äôll be getting the take home technical assessment by the end of the week. I kept waiting and got nothing, after  one and half weeks got a rejection email. 

I reached out to the recruiter to get the feedback and she said that the hiring manager says I have marketing experience and they want someone with data experience. I was like I literally don‚Äôt have any marketing experience. I‚Äôve been working as a data analyst, then sr data analyst and now analytics engineer. For background I‚Äôve 6.5 years of experience in data space, and no where in my resume did I mention anything about marketing nor did I say anything in the interview which would have caused this confusion.

","It seems more and more like everything is a matter of luck, sorry for your tough experience, better luck next time.  üí™ü¶æüìà","Last summer I interviewed for a position. Final round went well, got a call back from a recruiter saying ‚Äúwe loved you, you have both great experience in AI and Cyber‚Ä¶but we also wanted someone with Quantum experience as well‚Äù.

I almost lost it.

I got a job offer a month later. Don‚Äôt worry about it, let them fail on their own stupidity.","I was recently rejected by hiring company HR (applying for a data engineer role) for not having a ‚Äútechnical background‚Äù. Not sure what that means, the recruiter agency that found me is retained recruiter so they have a direct and priority line to HR and hiring manager, and the recruiter seems just as confused as I am. I have 16yrs total of experience in software engineering (have a computer engineering major and math minor) and 6yrs directly as a data engineer.

This specifically is for a data engineer as they are also hiring data science folks with a stats/ML background and that role has vastly different reqs."
1710621110.0,95,MOIRAI: A Revolutionary Time-Series Forecasting Foundation Model,"Salesforce released MOIRAI, a groundbreaking foundation TS model.  
The model code, weights and training dataset will be open-sourced.

You can find an analysis of the model [here](https://aihorizonforecast.substack.com/p/moirai-salesforces-foundation-transformer).

&#x200B;","Again, another published paper with dubious benchmarks. You exclude some models from one comparison then include them in the other comparison with different levels of detail‚Ä¶ this just brings more doubts than answers.. As always, we will have to wait for the released version to benchmark ourselves and see if it has good value.",[removed],I'm really tired of these sensationalist posts about foundational time series forecasting models. We already had TimeGPT which turned out to be nothing except over dramatic garbage to hype up their startup. People like to believe that time series can easily be reduced to a single base model like NLP but that likely unfortunately isn't the case. Time series is much more broad and wide-ranging than language.  We might have models that learn certain domains well using transfer learning but having a true foundation model is extremely unlikely given the breadth and dimensionality differences of time series data.
1708397979.0,93,How to version control Jupyter notebook?,"Many times  write a  long code and a major portion of the code needs to be changed, but I might need the old code for reference. Is there a way to version control the entire notebook?",Please (I can‚Äôt stress this enough) write functions/classes in .py files and use git,"https://jupytext.readthedocs.io/en/latest/

You‚Äôll need to put things in a script eventually, but when you‚Äôre in that awkward stage where you‚Äôre still experimenting but you‚Äôve written some reasonably hefty code this is golden. You can use git with the associated .py file to keep track of it.","Git!   
Here are things people commonly use for notebook version control with git -

* [nbdime](https://nbdime.readthedocs.io/en/latest/) to view local diffs & merge changes
* [jupytext](https://github.com/mwouts/jupytext) for 2-way sync between notebook & markdown/scripts
* [JupyterLab git](https://github.com/jupyterlab/jupyterlab-git) extension for git clone / pull / push & see visual diffs
* [Jupyerlab gitplus](https://github.com/ReviewNB/jupyterlab-gitplus) to create GitHub PRs from JupyterLab
* [ReviewNB](https://www.reviewnb.com/) for reviewing notebook PRs / Commits on GitHub

Disclaimer: I‚Äôm the author of last two (GitPlus & ReviewNB). But I‚Äôve presented the overall landscape of the most popular solutions in the notebook version control space."
1705046302.0,96,How to deal with data scientists,"I'm an R&D engineer working for an international company in the chemical/engineering business. I am the only one in my team with some data analytics skills. I typically spend about 20% of my time coding and analysing data for my projects (sometimes I get asked to support other colleagues too). Nothing fancy, some statistical analysis, a lot of data viz and sometimes simple modelling mostly for inferential purposes.
Over the years my company built a relatively large and very centralized data science team, they do the heavy lifting and create production quality code.  
In the early days I enjoyed dealing with them, they were happy that I could read the code and understand the lingo so we could cooperate very smoothly on the same project. Typically we went through their code together on a weekly basis. By the end of the project  I was aware of most of the technical decision made along the way and I could explain it pretty comfortably to the stakeholders. So much so that I started thing about a career switch. Now that team is gone and the newcomers are more senior and operate very differently. They typically provide their report on powerpoint and quite high level too, I had a few instances when I reached out and got dismissed. One particular time after they analysed some data for an RD&E project I asked the code specifying that I was really eager to learn and I got denied. Their justification was that they were afraid their code would be used the wrong way.  
They don't seem to be interested in cooperating the same way, they rather get the data, ask all the questions, and share their final findings. They do not interact much along the way.

This situation has been going on for over a year already, I thought I could find a sweet spot bridging between RD&E and data science, but I'm now having second thoughts and thinking I'm better off in my bubble. I'm very hesitant involving them in any project unless strictly necessary.   
I'm interested to hear some opinion from data scientists as I'm trying to put myself in their shoes. Am I being too intrusive? Have you ever had similar experiences from the opposite side? Also do you have any career advice? Is there any value for someone with a foot in both shoes?",what happened to the previous team? current one seems defensive & gate keeping,"There is absolutely value in having a foot in both shoes, as you put it. I think the most productive data science work is done when there is an understanding of how the analysis support decisions, and that means understanding how analysis converts into practice in the hands of the non-data scientist.

What you describe sounds like fairly standard institutional rot. It happens easily that departments within companies develop self-interests. A lot of standard management theory is how to deal with this and still make everyone pull in the same direction. So in some ways, what you describe is neither data science specific, nor is it something that is entirely in your power to adjust, since it is a question of group dynamics. I've seen this happen as startups grow from being small and scrappy to larger and internally fragmented.

That said, some things can be done on the margins. You describe the risk that data science teams become isolated and not involved. That is a ""stick"" that can induce change. A team that few see the value of is the first to go when downsizing happens. Is there someone in the data science team who can take that bigger perspective and understand that part of their self-interest is to be useful to others? You need someone on the other side to bridge build with. One person can be enough, you do not have to get the entire team to like you.

Trying to change the old ways of doing things through specialized projects could be a way to induce change. Say you identify a potential project, not too large in scope, that could involve the data science team, but in different ways than usual. The point is that you constrain the efforts such that the deliverable simply is not a PowerPoint presentation, but code that runs somewhere doing something. I am hand-waving since this requires creativity and domain-specific understanding. But as I argued above, I think what you describe is very typical stuff that happens when ways of doing things become settled. So change the way things are done, unsettled the old ways a bit.

Maybe also they have some good reasons to be dismissive of you. Perhaps some analysis has become sufficiently routine that it can be automated far more and all the decision-makers need is the bottom line finding. This is also part of becoming more professional, to speed up some stuff that have adequate solutions such that one can focus on value creation and problem-solving elsewhere in the company. This is not to say that you should stop being interested or inquisitive, only that there can be good reasons for you to move on and find more productive areas to get involved in. This is part of the give and take of teamwork and in a well-run company, there is acceptance that sometimes people challenge others in what they do and argue for change. Friction is part of business, especially if you look to go outside your comfort zone and the organizational boundaries.","this is my experience with my current team. they are _on the same team as me_ and refuse to collaborate. i often get a ‚Äúyou‚Äôre not a data scientist‚Äù or ‚Äúthis project is different from normal software engineering‚Äù vibe from them. in my experience working with this team for 3 years, when they don‚Äôt want to share there‚Äôs something to hide. for 2 years i tried to explain to the team that the reported metrics don‚Äôt play out in testing. i was thoroughly dismissed. at the end of those two years an independent audit was performed where the inference metrics bore out barely better than random. everyone was super surprised except for me and a handful of other engineers. 

TL;DR i start with a new team on Monday."
1709294716.0,95,What are the motivations of LinkedIn influencers?,"I‚Äôve blocked most of the names that post every day unless I find their content engaging. But it left me wondering, what are the motivations for people who spend what I assume is several hours every week posting non-stop. 

The posts themselves are not monetized. Do hourly consulting opportunities usually come this way? Some of these people are corporate employees so it‚Äôs not just consultants unless they are all trying for side money. Is it a long term play to secure more job opportunities in the future? Both? Other things they‚Äôre building and trying to sell? Just to create a massive but impersonal network? What are the various reasons people do this?

It takes me mental energy to make even 1 post every couple weeks, so I am always dumbfounded when some people pour so much energy into this when they‚Äôre already in a high paying career.","My guess is that most are trying to build an audience to eventually monetize other content, e.g. courses, bootcamps, paid newsletters, etc. As you said, it can also help open doors for new jobs/consulting opportunities.

I don‚Äôt have evidence of this, but it seems like some of the big names are getting paid for promotions. If you see a data science influencer randomly promoting some company‚Äôs ¬†‚ÄúLLM Workshop‚Äù, they‚Äôre probably getting paid.¬†","Besides the great points brought up by others;
Never underestimate how highly people value social credit and attention.",They want you to move on to their paid content or product. It's really that simple.
1704790355.0,94,Out of job - how to keep up SQL practise?,"Hi guys

I am out of job but still need to keep my SQL skills brushed up. I have setup python environment and have installed PostgreSQL.

But how can i practice interview level/type questions?

I am using free version of Stratascratch as of now.

Anyone know any other sources to practice?

&#x200B;

**Edit**: Wow, thanks a lot for the responses guys! ‚ù§Ô∏è‚ù§Ô∏è Didn't think it would be so huge!","Founder of [DataLemur](https://datalemur.com/questions) here - check out the site, has 200+ interview questions and an interactive coding environment! Recently put out a [free SQL tutorial](https://datalemur.com/sql-tutorial) as well in case the problems are too hard!","get a dataset you‚Äôre interested in ‚Äî i think the nba dataset on kaggle is fun.

study the dataset.

ask questions about the dataset and answer them with queries (e.g. what players have scored more than 500 points on at least 3 different teams? list the mean and median salary on the team that won the championship every year. list teams that had an average salary of higher than the championship team for each year). 

basically let your natural curiosity make you ask fun questions and use sql to get answers. this will also have the side effect of making you a better analyst and not just a sql monkey.","DataLemur, LeetCode, Stratascratch, Interview Query, ChatGPT

I can help you out further. DM"
1709995888.0,91,Unable to progress in my skillset,"I recently got a job as a data scientist, but the job is more about automation and building rule based scripts rather than data science...like not even enough statistics. 

I know I should study in my off hours, but I am stuck in the ***""I don't know what I don't know""-zone***. 

Not sure how to explain that, it basically is...

Most of my projects involve topics that I am comfortable with. Concepts like deep learning will be good to learn, but nowhere useful in my line of work/industry. At max, there will be NLP.

But again, I am confused here, what topics to study and replicate them.

&#x200B;

&#x200B;","I never studied on my own time and had a fine career in DS. Automation and rule-based scripting can solve a lot of problems in a business. I've seen very few practical applications for deep learning in most businesses, in contrast.

Basically, if you became a data scientist and expected that you'd spend a lot of time working on deep learning, you were not given an accurate picture of the bulk of the work done by the bulk of the data scientists out there.

https://shakoist.substack.com/p/why-business-data-science-irritates

https://ryxcommar.com/2022/11/27/goodbye-data-science",Gotta learn on your own. End of the day you can‚Äôt do everything. I get stuck making a PowerPoint or 2 each week instead of automation tools which I like building or creating models. It‚Äôs a job and pays the bills,Welcome to the real world.
1709924720.0,94,"I made a Python package for creating UpSet plots to visualize interacting sets, release v0.1.2 is available now!","&#x200B;

https://preview.redd.it/oyj3x3p3t5nc1.png?width=1280&format=png&auto=webp&s=1da23add505e6afe4fcd9bd2b04cfc292f29139e

# TLDR

**upsetty** is a Python package I built to create UpSet plots and visualize intersecting sets. You can use the project yourself by installing with:

    pip install upsetty 

Project GitHub Page: [https://github.com/eskin22/upsetty](https://github.com/eskin22/upsetty)

Project PyPI Page: [https://pypi.org/project/upsetty/](https://pypi.org/project/upsetty/)

# Background

Recently I received a work assignment where the business partners wanted us to analyze the overlap of users across different platforms within our digital ecosystem, with the ultimate goal of determining which platforms are underutilized or driving the most engagement.

When I was exploring the data, I realized I didn't have a great mechanism for visualizing set interactions, so I started looking into [UpSet plots](https://en.wikipedia.org/wiki/UpSet_Plot). I think these diagrams are a much more elegant way of visualizing overlapping sets than alternatives such as Venn and Euler diagrams. I consulted [this Medium article](https://medium.com/@narayanmahto/visualizing-intersecting-sets-upset-chart-in-python-cf72e4cad5b1) that purported to explain how to create these plots in Python, but the instructions seemed to have been ripped directly from the projects' GitHub pages, which have not been updated in several years.

[One project by Lex et. al 2014](https://upset.app/) seems to work fairly well, but it has that 'matplotlib-esque' look to it. In other words, it seems visually outdated. I like creating views with libraries like Plotly, because it has a more modern look and feel, but noticed there is no UpSet figure available in the figure factory. So, I decided to create my own.

# Introducing 'upsetty'

[upsetty](https://github.com/eskin22/upsetty) is a new Python package [available on PyPI](https://pypi.org/project/upsetty/) that you can use to create upset plots to visualize intersecting sets. It's built with Plotly, and you can change the formatting/color scheme to your liking.

&#x200B;

https://preview.redd.it/fzwyyx2hs5nc1.png?width=1747&format=png&auto=webp&s=b261316d37f48f1c07fc7d815ff7e537e20cb2e8

# Feedback

This is still a WIP, but I hope that it can help some of you who may have faced a similar issue with a lack of pertinent packages. **Any and all feedback is appreciated.** Thank you!","I work with sets a lot, and so I love this. Thanks for sharing",This is slick! Thanks man.,awesome!! great work
1706054056.0,92,"Correlation does not equal causation. But, does no correlation mean two variables can‚Äôt be causally related?","This is something I‚Äôve thought of time-to-time. We all know correlation != causation. But is it also true that some type of correlation must statistically exist if two variables are in fact causally related? Does the lack of correlation rule out causation? Can one thing cause another and go completely undetectable? 

I‚Äôm not well read on the philosophical schools of thought on causation, but my gut tells me that a lack of correlation would rule out causation. And this is an interesting because we can then rule out theories based on this rule - helping us get to the truth. Is this the wrong take?","No. Zero correlation does not imply no causation.

Suppose you are maintaining a constant speed of 40 mph while driving on a hilly road. On the uphill sections of the road, you are pressing hard on the accelerator to maintain 40 mph. On the downhill sections of the road you aren't pressing the accelerator at all (possibly breaking) to maintain 40 mph.

In this example, there is *no correlation* between how hard you are pressing on the accelerator and speed, as the speed stays constant. However, pressing on the accelerator still *causes* the car to go faster.","No; two variables could have a causal relationship, and a confounder could cancel out that relationship, making it appear that they are unrelated.

As a silly example, rain causes things to get wet. If I measure the wetness of my yard with and without rain I would see that causal relationship. But if I put an awning over my yard, measuring the wetness of my yard would no longer show the relationship because the awning is covering up the causal relationship.",No. You could have a third variable obscure the causal relationship.
1710809428.0,91,I hope it‚Äôs not just me‚Ä¶,"I‚Äôm definitely more on the data engineering/wrangling side, but I feel like meeting (more like guessing) end user specifications is one of the hardest parts of my job. But I had to lol when I saw != written like this. It‚Äôs actually kinda clever ü§£",[deleted],"HUGE EDIT

As someone pointed out, this is the way not equals is commonly written in SQL. I almost feel like removing this post hahaha. But really the discussion was meant to be about the non programming challenges of your job, if any.  I know it‚Äôs different for everyone, as the ‚Äúdata science‚Äù field is broad. Best way I can think of it is like you may have the fasted rower in the world and team him/her up with the best canoe builder in the world. But that doesn‚Äôt mean that they will win the rowing competition","I'm used to writing <> since proc SQL in SAS doesn't recognize !=. So, I've just gotten used to writing <>"
1705849368.0,96,Is medium subscription worth it? Any alternatives?,"My one complaint with data science articles on medium is the amount of noise with respect to quality content. other than that, I really like having an app that has a recommended feed of topics I‚Äôm interested in and a consistent reading experience in one app and I don‚Äôt mind the price. I subscribe to newsletters that have much better articles, but having to dig through multiple emails anytime I want to read something, then dealing with various blog formats bugs me.

Has anyone found a good mix of these things? Or found a way to cut through the noise on medium?

Update: I‚Äôve found what I was looking for with Omnivore (read it later app). I can set up RSS feeds AND newsletters, they will set up a fake email for you that integrates. You can save articles you want to read, add from urls, store notes and highlights on each article, access in phone or computer and it formats the text and removes adds. An interface like Medium, but sources I control. ",[removed],Use freedium website to open any article in medium,"Although I've found some helpful medium/towardsdatascience articles, I've come to realize a lot of bloggers are just like me - self taught. Not a bad thing inherently but it does lead to misguided or outdated suggestions for DS.

I've found way better resources scouring through the Bayesian/DS folks twitter and bookmarking people with github style blogs or books"
1705493649.0,89,How have LLMs come into your workflow as a data scientist?,"Title. Basically, want to know for the data scientists here, how much is knowledge of LLMs needed nowadays? By knowledge I mean a theoretical and good understanding of how these things work. And while we‚Äôre on the topic, how about I just get a list of some DL concepts every data scientist should know, whether it‚Äôs NLP, vision, whatever. This is for data scientist.

I come from MS statistics background so books like casella bergers stat inference, elements of stat learning, Bayesian data analysis and forecasting came first before I really dove into deep learning. Really the most I‚Äôve ‚Äúdove‚Äù into deep learning was by reading about how artificial networks work, CNNs work, and then attempted to do a CNN (I know, not LSTM, I read some papers justifying why CNN is appropriate)  time series classification project, which I just didn‚Äôt figure out and frankly gave up on cause I fit the elastic Net and a kernel smoother for  the time series classification and it trashed all over the CNN.","I use it to debug long sql code, or make graphs. Fastest use of my time to get what I need","Now when I have to find an obscure method in the infinite Panda's documentation, or not sure if I'm doing something correctly, rather than spending 3 hours figuring it out, I spend 10 minutes writing a prompt, 5 minutes reading and thinking, and 3 minutes double-checking if anything seems sus.

  
It's quite amazing, time-saving wise.","Executives keep talking about them and I nod and ignore their fantasy statements. They hired a real estate agent to manage ‚ÄúAI.‚Äù¬†

I use copilot."
1712158544.0,114,[Need advice]I Want to Leave MAANG a Month After Being Hired,"(Hope this is the right subreddit as many people post job-related/job-seeking questions. If not, I'm sorry, that's an honest mistake.)

Hello everyone. I was recently hired by one of the big corporations, and I would appreciate advice from the community.

I was offered a high-level DS manager position at one of the MAANG companies. During the hiring process, I discussed with the hiring manager that I would start as an individual contributor, focusing on helping to improve the recommendation engine. Several months later, I was supposed to transition into managing a small team. I was extremely excited, it‚Äôs MAANG, after all.

However, on my very first day of the job, my manager‚Äôs peer informed me that there had been a reorg. As a result, the tasks we had initially discussed would not be happening. To make matters worse, it turned out that my manager‚Äôs planned promotion didn‚Äôt materialize, leaving no team for me to manage. I thought, ‚ÄúOkay, I can live with this. Worst-case scenario, I‚Äôll transfer to another team later.‚Äù

Fast forward two weeks, and I honestly hate it. The tools we use are awful. Simple tasks that would take minutes outside of big tech now take up to an hour because our small databases constantly crash under load. Even building a basic causal model becomes incredibly challenging because we can‚Äôt extract data from the database for a sufficiently long period. When I raised these concerns with my manager, their response was, ‚ÄúI‚Äôm not a fan of complex modeling. If you want to do that, let‚Äôs outsource it to the Data Engineering (DE) team.‚Äù

What‚Äôs more, my daily work involves adding charts to dashboards and handling ad hoc requests from various stakeholders. Nobody can clearly state our long-term goals or what we‚Äôre trying to achieve. Our OKRs are DAU and Revenue, which are essentially driven by other company products (lol). The most disconcerting part is that every other Data Scientist in our part of the organization seems to be doing the same ‚Äî ad-hocs, reports, and dashboards. Frankly, this doesn‚Äôt feel like DS work at all.

In my previous workplace, I held the responsibility of creating models and introducing new ways to leverage data to increase our KPIs. My team was first to introduce causal modeling and ML-based user segmentation, we‚Äôve also piloted several other things that involved shipping models to production. When I decided to leave, my employer has repeated several times that should anything go wrong, I shouldn‚Äôt hesitate and come back to working for them. I‚Äôm seriously consider this option now. By the way it wasn't a small shop, that was an international company with petabytes of data and over a billion of revenue per year. Not the MAANG level though.

My current job offers better compensation, particularly if I can vest my stock options. However, during these several weeks, we‚Äôve already experienced another reorganization (3rd or 4th in 1.5 years), making it highly unlikely that I‚Äôll vest before the next round of layoffs.

What would be the right move? To those familiar with MAANG companies, I‚Äôd appreciate your insights. Is this situation normal?","Big Tech companies are huge, it seems like you‚Äôre caught in a bad team. I would wait some months and try an internal transfer. Your compensation is probably great  and the company looks great in a CV, so I‚Äôd be wary of jumping ship without anything better lined up. Is your old company something that will have better leverage on your career long term?","You're not missing anything. For reasons unclear to me a lot of tech company ""DS"" jobs are really analyst positions and can be surprisingly low on technical skill requirements. I did 18 months at one and it felt like my resume was rotting. Now I lead a team at non-tech company and can say I wouldn't hire any ds I worked with at the tech company not because they weren't good people but because they didn't have the scientific or technical skills ds need.

That doesn't mean you should leave, but I think it's a better idea than it would have sounded in 2020 or so.",You need to decide what‚Äôs most important to you; the clout of MAANG or the personal satisfaction of the other job.
1709903158.0,88,Anything that you guys suggest that I can do on my own to practice and build models?,"I‚Äôm not great at coding despite knowledge in them. But I recently found out that you can use Azure machine learning service to train models.


I‚Äôm wondering if there‚Äôs anything that you guys can suggest I do on my own for fun to practice. 

Anything in your own daily lives that you‚Äôve gathered data on and was able to get some insights on through data science tools? ","Like another commenter said, you should find a subject you‚Äôre interested in and commit to making a model to solve some problem in that domain. 

Like music? Use Spotify‚Äôs API to make a music recommendation system. Hate taking notes about how a bunch of sources relate to each other? Maybe you could use a clustering algorithm to try and identify latent similarities between them. The possibilities are endless.

If you get stuck, and I may catch some flak for this, but I‚Äôve personally found ChatGPT to be an incredibly powerful learning tool. If you get stuck on something, use it to teach you. Don‚Äôt just ask it to write all the code, but ask it questions about the approach. For example, if you‚Äôre trying to reduce the dimensionality of data and decide to use PCA, ask it about why and how PCA works. 

Try to build up an understanding with basics like linear regression, logistic regression, etc. and then you can build your way up to neural nets.

It may seem like neural nets are all the rage right now (which is true) but data science in a business context will be the simpler models 90% of the time and the big fancy things will be reserved for only the remaining 10%.

Just be curious. Brush up on math, learn the basics of Python, Pandas, etc. and find something to build that you‚Äôre passionate about.

Very best of luck!","Enterprise AI Engineer here, 15 years experience.

If you're not great at coding, and that's what you want to work on, don't waste your time on the modeling phase of projects. Such is (in my experience) 5% of the coding that's involved in a project's entirety, with the other 95% being cleaning/prepping the data. 

Instead, look for data cleaning exercises were you take multiple sources clean them and prepare them for a model.

As for ideas, as others mentioned, find something fun and run with it!","Visit Kaggle, there you can find many datasets with real world problems.

Once you are more prepared you can even join the competition and win some cash."
1709591312.0,84,What sort of extra activities/events do data scientists do?,"I‚Äôve heard about ML competitions and data engineering hackathons, as well as pitching and entrepreneurship contests that my computer science classmates talk about. There‚Äôs even the challenges, projects, and tools flair on this sub. 

What *all* is out there? I‚Äôm just a curious undergrad lol","I hang out with my girlfriend and do all sorts of other stuff like bicycling, camping, beach trips, fancy dinners, shows, visit museums, foster dogs, etc. I seem to earn many office politicking points sharing these activities with my peers in other departments and wield them to humanize myself and my activities which affords¬†me lots of agency despite low data literacy in the company. I won‚Äôt die regretting I didn‚Äôt live life, although I may die poor. Whatever.",Try to be a normal human,I see so many juniors talking about hackathon this or portfolio that. I just play video games.
1711059549.0,85,Is it OKAY if I simplified p-valur to non tech stakeholders to the extent it no longer a statistical term?,"I mean, can I tell the head of operations this: if we run the experiment again, it's only 2.1% chance that the differences we observed is by chance, if we run the experiment again, it's a 97.8% chance that we would get values as or more extreme than what we saw?

Can we even make it simpler?

Otherwise, if he heard the word hypothesis, he starts making ""fuck it I should have stayed home"" noise",Just say statistically significant the blanket term covers all,I don‚Äôt see how your explanation simplifies anything.¬†,‚ÄúWe should use Landing Page B. The math says the model is good.‚Äù
